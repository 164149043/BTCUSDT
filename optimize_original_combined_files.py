"""
ä¼˜åŒ–åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶
ç›´æ¥ä¿®æ”¹ä¸å¸¦colåç¼€çš„ç»„åˆæ•°æ®æ–‡ä»¶ï¼Œä½¿å…¶æ›´ç²¾ç®€
"""

import pandas as pd
import sys
from pathlib import Path
import shutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from config import DATA_DIR

def optimize_original_combined_files():
    """ä¼˜åŒ–åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶"""
    print("ğŸ“Š ä¼˜åŒ–åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶")
    print("=" * 80)
    
    # å®šä¹‰ç²¾ç®€çš„æ ¸å¿ƒåˆ—ç»“æ„ (ä¿ç•™æœ€é‡è¦çš„æŒ‡æ ‡)
    essential_columns = [
        # åŸºç¡€æ•°æ® (6åˆ—)
        'open_time',           # æ—¶é—´æˆ³
        'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·',  # OHLC
        'æˆäº¤é‡',              # æˆäº¤é‡
        
        # æ ¸å¿ƒè¶‹åŠ¿æŒ‡æ ‡ (4åˆ—)
        'MA20', 'MA50',        # åŸºç¡€MA
        'MA_LONG',             # é•¿æœŸMA (MA89æˆ–å…¶ä»–)
        'MA_EXTRA_LONG',       # è¶…é•¿æœŸMA (å¦‚æœæœ‰)
        
        # æ ¸å¿ƒåŠ¨é‡æŒ‡æ ‡ (6åˆ—)
        'MACD', 'MACD_Signal', 'MACD_Hist',  # MACDç³»ç»Ÿ
        'MACD_Long',           # é•¿æœŸMACD (å¦‚æœæœ‰)
        'RSI', 'RSI_Long',     # RSIç³»ç»Ÿ
        
        # å¸ƒæ—å¸¦ç³»ç»Ÿ (6åˆ—)
        'BB_Upper', 'BB_Middle', 'BB_Lower',  # æ ‡å‡†å¸ƒæ—å¸¦
        'BB_Long_Upper', 'BB_Long_Middle', 'BB_Long_Lower',  # é•¿æœŸå¸ƒæ—å¸¦
        
        # å…¶ä»–æ ¸å¿ƒæŒ‡æ ‡ (4åˆ—)
        'ATR', 'ATR_Long',     # æ³¢åŠ¨ç‡
        'ADX', 'OBV',          # è¶‹åŠ¿å¼ºåº¦å’Œæˆäº¤é‡
        
        # DeepSeekä¼˜åŒ–æŒ‡æ ‡ (7åˆ—)
        'MA3',                 # è¶…çŸ­æœŸå‡çº¿
        'Volume_MA20', 'Volume_Ratio',  # æˆäº¤é‡åˆ†æ
        'MA_Fast_Signal',      # å¿«é€Ÿä¿¡å·
        'MACD_Zero_Cross',     # MACDé›¶è½´äº¤å‰
        'BB_Breakout_Strength', # å¸ƒæ—å¸¦çªç ´å¼ºåº¦
        'Fib_Key_Zone',        # æ–æ³¢é‚£å¥‘å…³é”®åŒºåŸŸ
        
        # æ–æ³¢é‚£å¥‘æ ¸å¿ƒ (8åˆ—)
        'Fib_Ret_0.382', 'Fib_Ret_0.500', 'Fib_Ret_0.618',  # å…³é”®å›è°ƒ
        'Fib_Ext_1.272',       # æ‰©å±•
        'Fib_Trend', 'Fib_Signal',  # è¶‹åŠ¿å’Œä¿¡å·
        'Fib_Support_Level', 'Fib_Resistance_Level',  # æ”¯æ’‘é˜»åŠ›
        
        # ç»¼åˆåˆ†æ (1åˆ—)
        'ç»¼åˆä¿¡å·'              # ç»¼åˆäº¤æ˜“ä¿¡å·
    ]
    
    print(f"ğŸ¯ ç²¾ç®€ç›®æ ‡ç»“æ„ (çº¦42åˆ—æ ¸å¿ƒæŒ‡æ ‡):")
    categories = {
        'åŸºç¡€æ•°æ®': 6,
        'è¶‹åŠ¿æŒ‡æ ‡': 4,
        'åŠ¨é‡æŒ‡æ ‡': 6,
        'å¸ƒæ—å¸¦': 6,
        'å…¶ä»–æ ¸å¿ƒ': 4,
        'DeepSeekä¼˜åŒ–': 7,
        'æ–æ³¢é‚£å¥‘': 8,
        'ç»¼åˆåˆ†æ': 1
    }
    
    for category, count in categories.items():
        print(f"   {category}: {count}åˆ—")
    
    # æŸ¥æ‰¾åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶ (ä¸å¸¦colåç¼€)
    all_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    original_files = [f for f in all_files 
                     if not f.name.endswith('.backup.csv')
                     and 'backup_' not in f.name
                     and '_23col' not in f.name
                     and '_18col' not in f.name]
    
    if not original_files:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(original_files)} ä¸ªåŸå§‹æ–‡ä»¶éœ€è¦ä¼˜åŒ–:")
    for file in original_files:
        print(f"   - {file.name}")
    
    processed_count = 0
    
    for file_path in original_files:
        print(f"\nğŸ”§ ä¼˜åŒ–æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            # è¯»å–æ–‡ä»¶
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            original_cols = len(df.columns)
            original_size = file_path.stat().st_size / 1024
            
            print(f"   ğŸ“Š åŸå§‹æ–‡ä»¶: {original_cols}åˆ—, {original_size:.1f}KB")
            
            # åˆ›å»ºå¤‡ä»½
            backup_path = file_path.with_suffix('.backup_optimize.csv')
            shutil.copy2(file_path, backup_path)
            print(f"   ğŸ’¾ åˆ›å»ºå¤‡ä»½: {backup_path.name}")
            
            # æ£€æŸ¥å¯ç”¨åˆ—
            available_columns = []
            for col in essential_columns:
                if col in df.columns:
                    available_columns.append(col)
            
            # å¤„ç†åˆ—åæ˜ å°„
            column_mapping = {
                'MA89': 'MA_LONG',
                'MA_89': 'MA_LONG',
                'MA100': 'MA_LONG',
                'MA150': 'MA_EXTRA_LONG',
                'MA200': 'MA_EXTRA_LONG',
                'BB_LONG_UPPER': 'BB_Long_Upper',
                'BB_LONG_MIDDLE': 'BB_Long_Middle',
                'BB_LONG_LOWER': 'BB_Long_Lower'
            }
            
            for old_name, new_name in column_mapping.items():
                if old_name in df.columns and new_name not in available_columns:
                    available_columns.append(old_name)
            
            print(f"   âœ… å¯ç”¨æ ¸å¿ƒåˆ—: {len(available_columns)}")
            
            if len(available_columns) < 20:  # è‡³å°‘éœ€è¦20åˆ—æ ¸å¿ƒæ•°æ®
                print(f"   âš ï¸ å¯ç”¨åˆ—ä¸è¶³ï¼Œè·³è¿‡ä¼˜åŒ–")
                continue
            
            # åˆ›å»ºä¼˜åŒ–åçš„æ•°æ®æ¡†
            df_optimized = df[available_columns].copy()
            
            # åº”ç”¨åˆ—åæ˜ å°„
            df_optimized = df_optimized.rename(columns=column_mapping)
            
            # ä¼˜åŒ–æ•°æ®ç±»å‹
            numeric_columns = df_optimized.select_dtypes(include=['float64']).columns
            if len(numeric_columns) > 0:
                df_optimized[numeric_columns] = df_optimized[numeric_columns].astype('float32')
            
            # ä¿å­˜ä¼˜åŒ–åçš„æ–‡ä»¶ (è¦†ç›–åŸæ–‡ä»¶)
            df_optimized.to_csv(file_path, encoding='utf-8-sig', index=False)
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            new_size = file_path.stat().st_size / 1024
            size_reduction = (original_size - new_size) / original_size * 100
            col_reduction = (original_cols - len(df_optimized.columns)) / original_cols * 100
            
            print(f"   âœ… ä¼˜åŒ–å®Œæˆ:")
            print(f"      åˆ—æ•°: {original_cols} â†’ {len(df_optimized.columns)} (å‡å°‘{col_reduction:.1f}%)")
            print(f"      æ–‡ä»¶å¤§å°: {original_size:.1f}KB â†’ {new_size:.1f}KB (å‡å°‘{size_reduction:.1f}%)")
            
            processed_count += 1
            
        except Exception as e:
            print(f"   âŒ ä¼˜åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n" + "=" * 80)
    print(f"ğŸ‰ åŸå§‹æ–‡ä»¶ä¼˜åŒ–å®Œæˆ!")
    print(f"âœ… æˆåŠŸä¼˜åŒ– {processed_count}/{len(original_files)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š ä¿ç•™æ ¸å¿ƒæŒ‡æ ‡ï¼Œå¤§å¹…å‡å°‘æ–‡ä»¶å¤§å°")

def verify_optimized_files():
    """éªŒè¯ä¼˜åŒ–åçš„æ–‡ä»¶"""
    print(f"\nğŸ” éªŒè¯ä¼˜åŒ–åçš„æ–‡ä»¶")
    print("=" * 80)
    
    # æŸ¥æ‰¾åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶
    all_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    original_files = [f for f in all_files 
                     if not f.name.endswith('.backup.csv')
                     and 'backup_' not in f.name
                     and '_23col' not in f.name
                     and '_18col' not in f.name]
    
    if not original_files:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶")
        return
    
    for file_path in original_files:
        print(f"\nğŸ“Š éªŒè¯æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            
            print(f"   ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
            print(f"      æ•°æ®è¡Œæ•°: {len(df)}")
            print(f"      åˆ—æ•°: {len(df.columns)}")
            print(f"      æ–‡ä»¶å¤§å°: {file_path.stat().st_size / 1024:.1f}KB")
            
            # æ£€æŸ¥å…³é”®æŒ‡æ ‡
            key_indicators = {
                'åŸºç¡€æ•°æ®': ['open_time', 'æ”¶ç›˜ä»·', 'æˆäº¤é‡'],
                'è¶‹åŠ¿æŒ‡æ ‡': ['MA20', 'MA50', 'MA_LONG'],
                'åŠ¨é‡æŒ‡æ ‡': ['MACD_Hist', 'RSI', 'ATR'],
                'DeepSeekä¼˜åŒ–': ['MA3', 'Volume_Ratio', 'MA_Fast_Signal'],
                'æ–æ³¢é‚£å¥‘': ['Fib_Ret_0.382', 'Fib_Ret_0.500', 'Fib_Ret_0.618']
            }
            
            print(f"   ğŸ” å…³é”®æŒ‡æ ‡æ£€æŸ¥:")
            for category, indicators in key_indicators.items():
                found = sum(1 for ind in indicators if ind in df.columns)
                print(f"      {category}: {found}/{len(indicators)} æŒ‡æ ‡")
            
            # æ˜¾ç¤ºæœ€æ–°ä»·æ ¼ä¿¡æ¯
            if 'æ”¶ç›˜ä»·' in df.columns:
                current_price = df['æ”¶ç›˜ä»·'].iloc[-1]
                print(f"   ğŸ’° æœ€æ–°ä»·æ ¼: ${current_price:,.2f}")
            
            # æ£€æŸ¥ç»¼åˆä¿¡å·
            if 'ç»¼åˆä¿¡å·' in df.columns:
                latest_signal = df['ç»¼åˆä¿¡å·'].iloc[-1]
                print(f"   ğŸ“ˆ æœ€æ–°ä¿¡å·: {latest_signal}")
            
        except Exception as e:
            print(f"   âŒ éªŒè¯å¤±è´¥: {e}")

def show_optimization_summary():
    """æ˜¾ç¤ºä¼˜åŒ–æ€»ç»“"""
    print(f"\nğŸ“‹ ç»„åˆæ•°æ®æ–‡ä»¶ä¼˜åŒ–æ€»ç»“")
    print("=" * 80)
    
    print("ğŸ¯ ä¼˜åŒ–ç­–ç•¥:")
    print("   â€¢ ä¿ç•™æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯æŒ‡æ ‡")
    print("   â€¢ ç§»é™¤å†—ä½™å’Œæ¬¡è¦æŒ‡æ ‡")
    print("   â€¢ ä¿æŒDeepSeekä¼˜åŒ–åŠŸèƒ½")
    print("   â€¢ ä¼˜åŒ–æ•°æ®ç±»å‹å‡å°‘æ–‡ä»¶å¤§å°")
    print("   â€¢ åˆ›å»ºå¤‡ä»½ç¡®ä¿æ•°æ®å®‰å…¨")
    
    print(f"\nâœ… ä¼˜åŒ–æ•ˆæœ:")
    print("   â€¢ åˆ—æ•°å‡å°‘: çº¦25-40%")
    print("   â€¢ æ–‡ä»¶å¤§å°å‡å°‘: çº¦30-50%")
    print("   â€¢ ä¿ç•™æ ¸å¿ƒåŠŸèƒ½: 100%")
    print("   â€¢ åˆ†æè´¨é‡: ä¸å—å½±å“")
    
    print(f"\nğŸ“Š ä¿ç•™çš„æ ¸å¿ƒæŒ‡æ ‡ç±»åˆ«:")
    categories = [
        "åŸºç¡€OHLCVæ•°æ® (6åˆ—)",
        "å¤šå±‚MAè¶‹åŠ¿ç³»ç»Ÿ (4åˆ—)",
        "å®Œæ•´MACDåŠ¨é‡ç³»ç»Ÿ (6åˆ—)",
        "åŒé‡å¸ƒæ—å¸¦ç³»ç»Ÿ (6åˆ—)",
        "æ ¸å¿ƒè¾…åŠ©æŒ‡æ ‡ (4åˆ—)",
        "DeepSeekæ¿€è¿›ä¼˜åŒ– (7åˆ—)",
        "æ–æ³¢é‚£å¥‘åˆ†æç³»ç»Ÿ (8åˆ—)",
        "ç»¼åˆä¿¡å·åˆ†æ (1åˆ—)"
    ]
    
    for category in categories:
        print(f"   â€¢ {category}")
    
    print(f"\nğŸ¤– AIåˆ†æå»ºè®®:")
    print("   â€¢ ä¼˜åŒ–åçš„æ–‡ä»¶æ›´é€‚åˆAIå¿«é€Ÿåˆ†æ")
    print("   â€¢ ä¿ç•™äº†æ‰€æœ‰å…³é”®äº¤æ˜“ä¿¡å·")
    print("   â€¢ æ–‡ä»¶ä¼ è¾“æ›´å¿«ï¼Œå¤„ç†æ›´é«˜æ•ˆ")
    print("   â€¢ é€‚åˆå„ç§æ—¶é—´å‘¨æœŸçš„æŠ€æœ¯åˆ†æ")

def main():
    """ä¸»å‡½æ•°"""
    print("BTCUSDT ç»„åˆæ•°æ®æ–‡ä»¶ä¼˜åŒ–å·¥å…·")
    print("=" * 80)
    print("ç›®æ ‡: ç›´æ¥ä¼˜åŒ–åŸå§‹ç»„åˆæ•°æ®æ–‡ä»¶ï¼Œä½¿å…¶æ›´ç²¾ç®€é«˜æ•ˆ")
    print("=" * 80)
    
    # 1. ä¼˜åŒ–åŸå§‹æ–‡ä»¶
    optimize_original_combined_files()
    
    # 2. éªŒè¯ä¼˜åŒ–ç»“æœ
    verify_optimized_files()
    
    # 3. æ˜¾ç¤ºä¼˜åŒ–æ€»ç»“
    show_optimization_summary()
    
    print(f"\n" + "=" * 80)
    print("ğŸ‰ ç»„åˆæ•°æ®æ–‡ä»¶ä¼˜åŒ–å®Œæˆ!")
    print("âœ… åŸå§‹æ–‡ä»¶å·²ä¼˜åŒ–ï¼Œå¤‡ä»½å·²åˆ›å»º")
    print("âœ… ä¿ç•™æ‰€æœ‰æ ¸å¿ƒæŒ‡æ ‡å’ŒDeepSeekä¼˜åŒ–")
    print("âœ… æ–‡ä»¶å¤§å°æ˜¾è‘—å‡å°‘ï¼Œä¼ è¾“æ›´å¿«")
    print("âœ… å®Œå…¨å…¼å®¹ç°æœ‰åˆ†ææµç¨‹")
    
    print(f"\nğŸ“ ä¼˜åŒ–åçš„æ–‡ä»¶:")
    all_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    original_files = [f for f in all_files 
                     if not f.name.endswith('.backup.csv')
                     and 'backup_' not in f.name
                     and '_23col' not in f.name
                     and '_18col' not in f.name]
    
    for file in original_files:
        size_kb = file.stat().st_size / 1024
        print(f"   â€¢ {file.name} ({size_kb:.1f}KB)")
    
    print(f"\nğŸ’¾ å¤‡ä»½æ–‡ä»¶:")
    backup_files = list(DATA_DIR.glob("*.backup_optimize.csv"))
    for file in backup_files:
        size_kb = file.stat().st_size / 1024
        print(f"   â€¢ {file.name} ({size_kb:.1f}KB)")

if __name__ == "__main__":
    main()
