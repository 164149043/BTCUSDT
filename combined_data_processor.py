"""
ç»„åˆæ•°æ®å¤„ç†æ¨¡å—
åŠŸèƒ½ï¼šåˆå¹¶åŸå§‹æ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼Œç”ŸæˆåŒ…å«å®Œæ•´ä¿¡æ¯çš„CSVæ–‡ä»¶
è¾“å‡ºï¼šåŒ…å«æ—¥çº¿åŸå§‹æ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡çš„åˆå¹¶æ•°æ®é›†
"""

import pandas as pd
import os
import sys
from pathlib import Path
from datetime import datetime
from config import DATA_DIR, RAW_DATA_FILENAME, INDICATORS_FILENAME, COMBINED_FILENAME, SYMBOL, get_filenames

def combine_data(raw_filename=None, indicators_filename=None, combined_filename=None, timeframe_name=None):
    """
    ä¸»å‡½æ•°ï¼šåˆå¹¶åŸå§‹æ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡æ•°æ®
    å‚æ•°:
        raw_filename: åŸå§‹æ•°æ®æ–‡ä»¶å
        indicators_filename: æŒ‡æ ‡æ•°æ®æ–‡ä»¶å
        combined_filename: ç»„åˆæ•°æ®æ–‡ä»¶å
        timeframe_name: æ—¶é—´å‘¨æœŸåç§°
    è¿”å›:
        Path: åˆå¹¶åæ–‡ä»¶çš„è·¯å¾„å¯¹è±¡
        None: å¦‚æœåˆå¹¶å¤±è´¥
    """
    print("\n" + "=" * 50)
    print(f"å¼€å§‹åˆå¹¶åŸå§‹æ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡æ•°æ® - {timeframe_name or 'æ—¥çº¿'}")
    print("=" * 50)

    # 1. ç¡®å®šæ–‡ä»¶è·¯å¾„
    raw_path = DATA_DIR / (raw_filename or RAW_DATA_FILENAME)
    if not raw_path.exists():
        print(f"âŒ é”™è¯¯: åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {raw_path}")
        return None

    try:
        raw_df = pd.read_csv(raw_path, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸåŠ è½½åŸå§‹æ•°æ®, å…± {len(raw_df)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ åŠ è½½åŸå§‹æ•°æ®å¤±è´¥: {e}")
        return None

    # 2. åŠ è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®
    indicators_path = DATA_DIR / (indicators_filename or INDICATORS_FILENAME)
    if not indicators_path.exists():
        print(f"âŒ é”™è¯¯: æŠ€æœ¯æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨ - {indicators_path}")
        return None

    try:
        indicators_df = pd.read_csv(indicators_path, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸåŠ è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®, å…± {len(indicators_df)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ åŠ è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
        return None

    # 3. æ•°æ®é¢„å¤„ç†
    print("ğŸ”„ æ•°æ®é¢„å¤„ç†ä¸­...")

    # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå…¼å®¹ä¸åŒåˆ—åï¼‰
    time_col = None
    for col in ['open_time', 'æ—¥æœŸ', 'æ—¶é—´', 'timestamp']:
        if col in raw_df.columns and col in indicators_df.columns:
            time_col = col
            break

    if not time_col:
        print("âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°å…±åŒçš„æ—¶é—´åˆ—")
        return None

    # ç»Ÿä¸€æ—¶é—´æ ¼å¼
    for df in [raw_df, indicators_df]:
        df[time_col] = pd.to_datetime(df[time_col])
        df.sort_values(time_col, inplace=True)
        df.reset_index(drop=True, inplace=True)

    # 4. åˆå¹¶æ•°æ®
    print("ğŸ”€ åˆå¹¶æ•°æ®ä¸­...")

    # è¯†åˆ«é‡å¤åˆ—ï¼ˆé™¤äº†æ—¶é—´åˆ—ï¼‰
    duplicate_cols = [col for col in indicators_df.columns
                     if col in raw_df.columns and col != time_col]

    if duplicate_cols:
        print(f"â– ç§»é™¤é‡å¤åˆ—: {', '.join(duplicate_cols)}")
        indicators_df = indicators_df.drop(columns=duplicate_cols)

    # æ‰§è¡Œåˆå¹¶ï¼ˆå†…è¿æ¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼‰
    try:
        combined_df = pd.merge(
            raw_df,
            indicators_df,
            on=time_col,
            how='inner',
            validate='one_to_one'
        )

        # å®šä¹‰è¦ç§»é™¤çš„åˆ— (æ¸…ç†å¤šä½™å’Œä¸­é—´è®¡ç®—æ•°æ®)
        columns_to_remove = [
            # ä¿¡å·åˆ†æåˆ— (æ–‡æœ¬ä¿¡å·ï¼Œéæ•°å€¼æ•°æ®)
            'è®¡ç®—æ—¶é—´',
            'MA_Signal',
            'MACD_Signal_Analysis',
            'RSI_Signal',
            'BB_Signal',
            'Stoch_Signal',
            'ç»¼åˆä¿¡å·',

            # ä¸­é—´è®¡ç®—æ•°æ® (éæ ¸å¿ƒæŒ‡æ ‡)
            'BB_Squeeze',           # å¸ƒæ—å¸¦æŒ¤å‹æ ‡å¿— (æ‚¨è¦æ±‚ç§»é™¤)
            'BB_Width',             # å¸ƒæ—å¸¦å®½åº¦ (ä¸­é—´è®¡ç®—æ•°æ®)

            # é‡å¤çš„MAåˆ— (ä¿ç•™æ ‡å‡†å‘½å)
            'MA8', 'MA21', 'MA55',  # ç§»é™¤åŠ¨æ€å‘½åçš„MAï¼Œä¿ç•™MA20, MA50, MA_LONG

            # å…¶ä»–å¯èƒ½çš„å¤šä½™åˆ—
            'MACD_Long_Hist',       # å¦‚æœå­˜åœ¨é•¿æœŸMACDæŸ±çŠ¶å›¾
            'RSI_Extra_Long',       # å¦‚æœå­˜åœ¨è¶…é•¿æœŸRSIä¸”ä¸éœ€è¦
        ]

        # ç§»é™¤ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
        existing_columns_to_remove = [col for col in columns_to_remove if col in combined_df.columns]
        if existing_columns_to_remove:
            combined_df = combined_df.drop(columns=existing_columns_to_remove)
            print(f"ğŸ—‘ï¸ å·²ç§»é™¤åˆ—: {', '.join(existing_columns_to_remove)}")

        # æ•°æ®æ¸…ç†å’ŒéªŒè¯
        combined_df = clean_and_validate_data(combined_df)

        # æŒ‰æ—¶é—´æ’åº
        combined_df.sort_values(time_col, ascending=True, inplace=True)
        combined_df.reset_index(drop=True, inplace=True)
    except Exception as e:
        print(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {e}")
        return None

    # 5. ä¿å­˜ç»“æœ
    combined_path = DATA_DIR / (combined_filename or COMBINED_FILENAME)

    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        combined_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºCSVï¼ˆå…¼å®¹ä¸­æ–‡Excelï¼‰
        combined_df.to_csv(combined_path, encoding='utf-8-sig', index=False)

        print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ! æ–‡ä»¶ä¿å­˜è‡³: {combined_path}")
        print(f"ğŸ“Š åˆå¹¶åæ•°æ®ç»´åº¦: {len(combined_df)} è¡Œ Ã— {len(combined_df.columns)} åˆ—")

        return combined_path
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        return None

def clean_and_validate_data(df):
    """
    æ¸…ç†å’ŒéªŒè¯ç»„åˆæ•°æ®
    å‚æ•°:
        df: ç»„åˆåçš„DataFrame
    è¿”å›:
        DataFrame: æ¸…ç†åçš„æ•°æ®
    """
    print("ğŸ§¹ æ•°æ®æ¸…ç†å’ŒéªŒè¯ä¸­...")

    # 1. æ£€æŸ¥é‡å¤åˆ—
    duplicate_columns = df.columns[df.columns.duplicated()].tolist()
    if duplicate_columns:
        print(f"âš ï¸ å‘ç°é‡å¤åˆ—å: {duplicate_columns}")
        df = df.loc[:, ~df.columns.duplicated()]
        print("âœ… å·²ç§»é™¤é‡å¤åˆ—")

    # 2. æ£€æŸ¥ç©ºå€¼è¿‡å¤šçš„åˆ— (è¶…è¿‡50%ä¸ºç©ºå€¼çš„åˆ—)
    null_percentage = df.isnull().sum() / len(df)
    high_null_columns = null_percentage[null_percentage > 0.5].index.tolist()
    if high_null_columns:
        print(f"âš ï¸ å‘ç°é«˜ç©ºå€¼åˆ— (>50%): {high_null_columns}")
        # å¯é€‰æ‹©ç§»é™¤æˆ–ä¿ç•™ï¼Œè¿™é‡Œé€‰æ‹©ä¿ç•™ä½†ç»™å‡ºè­¦å‘Š

    # 3. æ ‡å‡†åŒ–åˆ—åé¡ºåº (å°†é‡è¦åˆ—æ”¾åœ¨å‰é¢)
    preferred_order = [
        'open_time',           # æ—¶é—´
        'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·',  # OHLC
        'æˆäº¤é‡', 'æˆäº¤é¢', 'æˆäº¤ç¬”æ•°',          # æˆäº¤é‡æ•°æ®
        'ä¸»åŠ¨ä¹°å…¥é‡', 'ä¸»åŠ¨ä¹°å…¥é¢',             # ä¹°å…¥æ•°æ®
        'MA20', 'MA50', 'MA_LONG',            # ç§»åŠ¨å¹³å‡çº¿
        'MACD', 'MACD_Signal', 'MACD_Hist',   # MACD
        'RSI', 'RSI_Secondary', 'RSI_Long',   # RSIç³»åˆ—
        'BB_Upper', 'BB_Middle', 'BB_Lower',  # å¸ƒæ—å¸¦
        'BB_Long_Upper', 'BB_Long_Middle', 'BB_Long_Lower',  # é•¿æœŸå¸ƒæ—å¸¦
        'Stoch_SlowK', 'Stoch_SlowD',         # éšæœºæŒ‡æ ‡
        'OBV',                                # æˆäº¤é‡æŒ‡æ ‡
        'ATR', 'ATR_Long', 'ATR_Ratio',       # ATRç³»åˆ—
        'ADX',                                # è¶‹åŠ¿æŒ‡æ ‡
        # æ–æ³¢é‚£å¥‘æ°´å¹³ (æŒ‰é‡è¦æ€§æ’åº)
        'Fib_Ret_0.382', 'Fib_Ret_0.500', 'Fib_Ret_0.618',  # å…³é”®å›è°ƒæ°´å¹³
        'Fib_Ret_0.236', 'Fib_Ret_0.786', 'Fib_Ret_0.000', 'Fib_Ret_1.000',  # å…¶ä»–å›è°ƒæ°´å¹³
        'Fib_Ext_1.272', 'Fib_Ext_1.414',    # ä¿ç•™çš„æ‰©å±•æ°´å¹³ (ç§»é™¤1.618, 2.0, 2.618)
        'Fib_Trend', 'Fib_High', 'Fib_Low',  # æ–æ³¢é‚£å¥‘è¶‹åŠ¿å’Œå…³é”®ç‚¹
        'Fib_Signal', 'Fib_Support_Level', 'Fib_Resistance_Level', 'Fib_Price_Position'  # æ–æ³¢é‚£å¥‘ä¿¡å·
    ]

    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    existing_preferred = [col for col in preferred_order if col in df.columns]
    other_columns = [col for col in df.columns if col not in preferred_order]
    new_column_order = existing_preferred + other_columns

    df = df[new_column_order]
    print(f"âœ… åˆ—é¡ºåºå·²ä¼˜åŒ–ï¼Œæ ¸å¿ƒæŒ‡æ ‡å‰ç½®")

    # 4. æ•°æ®ç±»å‹ä¼˜åŒ–
    numeric_columns = df.select_dtypes(include=['float64']).columns
    if len(numeric_columns) > 0:
        # å°†float64è½¬æ¢ä¸ºfloat32ä»¥èŠ‚çœå†…å­˜ï¼Œä½†ä¿ç•™æ—¶é—´åˆ—ä¸ºåŸå§‹ç±»å‹
        time_columns = ['open_time']
        numeric_columns_to_convert = [col for col in numeric_columns if col not in time_columns]
        if numeric_columns_to_convert:
            df[numeric_columns_to_convert] = df[numeric_columns_to_convert].astype('float32')
            print(f"âœ… å·²ä¼˜åŒ–{len(numeric_columns_to_convert)}ä¸ªæ•°å€¼åˆ—çš„æ•°æ®ç±»å‹ (float64â†’float32)")

    # 5. æœ€ç»ˆéªŒè¯
    print(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

    return df

def display_combined_data_preview(file_path, num_rows=5):
    """
    æ˜¾ç¤ºåˆå¹¶æ•°æ®çš„é¢„è§ˆä¿¡æ¯
    å‚æ•°:
        file_path (Path): æ•°æ®æ–‡ä»¶è·¯å¾„
        num_rows (int): æ˜¾ç¤ºçš„è¡Œæ•°ï¼ˆé¦–å°¾å„æ˜¾ç¤ºnum_rowsè¡Œï¼‰
    """
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')

        print("\n" + "=" * 50)
        print(f"åˆå¹¶æ•°æ®é¢„è§ˆ ({file_path.name})")
        print("=" * 50)

        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        print(f"â— æ—¶é—´èŒƒå›´: {df.iloc[0, 0]} è‡³ {df.iloc[-1, 0]}")
        print(f"â— æ€»è®°å½•æ•°: {len(df)}")
        print(f"â— æ•°æ®åˆ—æ•°: {len(df.columns)}")

        # é€‰æ‹©å…³é”®åˆ—æ˜¾ç¤º
        key_columns = [
            col for col in [
                'open_time', 'æ—¥æœŸ', 'æ—¶é—´', 'timestamp',
                'å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡',
                'MA20', 'MA50', 'RSI', 'MACD', 'MACD_Signal', 'BB_Upper', 'BB_Lower'
            ] if col in df.columns
        ]

        # æ˜¾ç¤ºé¦–å°¾æ•°æ®
        if len(df) > num_rows * 2:
            preview = pd.concat([df.head(num_rows), df.tail(num_rows)])
            print(f"\nã€é¦–{num_rows}è¡Œ + å°¾{num_rows}è¡Œã€‘:")
        else:
            preview = df
            print("\nã€å…¨éƒ¨æ•°æ®ã€‘:")

        print(preview[key_columns].to_string(index=False))

        # æ˜¾ç¤ºåˆ—åæ¸…å•
        print("\nã€å…¨éƒ¨åˆ—åã€‘:")
        print(", ".join(df.columns))

    except Exception as e:
        print(f"âŒ æ•°æ®é¢„è§ˆå¤±è´¥: {e}")

def get_latest_combined_path():
    """è·å–æœ€æ–°çš„ç»„åˆæ•°æ®æ–‡ä»¶è·¯å¾„"""
    combined_path = DATA_DIR / COMBINED_FILENAME
    return combined_path if combined_path.exists() else None

if __name__ == "__main__":
    print("=" * 50)
    print("ç»„åˆæ•°æ®å¤„ç†æ¨¡å—æµ‹è¯•")
    print("=" * 50)

    try:
        # æ‰§è¡Œæ•°æ®åˆå¹¶
        result_path = combine_data()

        # æ˜¾ç¤ºé¢„è§ˆ
        if result_path:
            display_combined_data_preview(result_path)

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            df = pd.read_csv(result_path, encoding='utf-8-sig')
            required_cols = ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'MA20', 'MA50', 'RSI']
            missing_cols = [col for col in required_cols if col not in df.columns]

            if missing_cols:
                print(f"\nâš ï¸ è­¦å‘Š: ç¼ºå°‘å…³é”®åˆ— - {missing_cols}")
            else:
                print("\nâœ… æ•°æ®éªŒè¯é€šè¿‡")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()