"""
åˆ›å»º23åˆ—ç²¾ç®€ç»„åˆæ•°æ®æ–‡ä»¶
åªä¿ç•™ç”¨æˆ·æŒ‡å®šçš„23ä¸ªæ ¸å¿ƒåˆ—
"""

import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from config import DATA_DIR

def create_23_column_files():
    """åˆ›å»º23åˆ—ç²¾ç®€æ•°æ®æ–‡ä»¶"""
    print("ğŸ“Š åˆ›å»º23åˆ—ç²¾ç®€ç»„åˆæ•°æ®æ–‡ä»¶")
    print("=" * 80)
    
    # å®šä¹‰ç”¨æˆ·æŒ‡å®šçš„23åˆ—
    required_columns = [
        'open_time',           # 1. æ—¶é—´æˆ³
        'å¼€ç›˜ä»·',              # 2. å¼€ç›˜ä»·
        'æœ€é«˜ä»·',              # 3. æœ€é«˜ä»·
        'æœ€ä½ä»·',              # 4. æœ€ä½ä»·
        'æ”¶ç›˜ä»·',              # 5. æ”¶ç›˜ä»·
        'æˆäº¤é‡',              # 6. æˆäº¤é‡
        'MA20',               # 7. MA20
        'MA50',               # 8. MA50
        'MA89',               # 9. MA89 (æˆ–MA_LONG)
        'BB_Upper',           # 10. BB_Upper
        'BB_Lower',           # 11. BB_Lower
        'BB_Long_Upper',      # 12. BB_Long_Upper
        'BB_Long_Lower',      # 13. BB_Long_Lower
        'MACD_Hist',          # 14. MACD_Hist
        'RSI',                # 15. RSI
        'ATR',                # 16. ATR
        'Fib_Ret_0.382',      # 17. Fib_Ret_0.382
        'Fib_Ret_0.500',      # 18. Fib_Ret_0.500
        'Fib_Ret_0.618',      # 19. Fib_Ret_0.618
        'Fib_Support_Level',  # 20. Fib_Support_Level
        'Fib_Resistance_Level', # 21. Fib_Resistance_Level
        'Fib_Price_Position', # 22. Fib_Price_Position
        'MACD_Long'           # 23. MACD_Long
    ]
    
    print(f"ğŸ¯ ç›®æ ‡: åˆ›å»ºåŒ…å«ä»¥ä¸‹23åˆ—çš„ç²¾ç®€æ–‡ä»¶:")
    for i, col in enumerate(required_columns, 1):
        print(f"   {i:2d}. {col}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»„åˆæ•°æ®æ–‡ä»¶
    all_files = []
    all_files.extend(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    all_files.extend(DATA_DIR.glob("*_enhanced.csv"))
    all_files.extend(DATA_DIR.glob("*_optimized.csv"))
    
    # æ’é™¤å¤‡ä»½æ–‡ä»¶å’Œå·²ç»æ˜¯23åˆ—çš„æ–‡ä»¶
    files_to_process = [f for f in all_files if not f.name.endswith('.backup.csv') 
                       and 'backup_' not in f.name and '_23col' not in f.name]
    
    if not files_to_process:
        print("âŒ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†:")
    for file in files_to_process:
        print(f"   - {file.name}")
    
    processed_count = 0
    
    for file_path in files_to_process:
        print(f"\nğŸ”§ å¤„ç†æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            # è¯»å–æ–‡ä»¶
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            original_cols = len(df.columns)
            
            # æ£€æŸ¥åˆ—åæ˜ å°„ (å¤„ç†å¯èƒ½çš„åˆ—åå·®å¼‚)
            column_mapping = {}
            available_columns = []
            missing_columns = []
            
            for required_col in required_columns:
                if required_col in df.columns:
                    available_columns.append(required_col)
                else:
                    # æ£€æŸ¥å¯èƒ½çš„æ›¿ä»£åˆ—å
                    alternatives = get_alternative_column_names(required_col, df.columns)
                    if alternatives:
                        column_mapping[alternatives[0]] = required_col
                        available_columns.append(alternatives[0])
                        print(f"   ğŸ“ åˆ—åæ˜ å°„: {alternatives[0]} â†’ {required_col}")
                    else:
                        missing_columns.append(required_col)
            
            if missing_columns:
                print(f"   âš ï¸ ç¼ºå¤±åˆ—: {missing_columns}")
            
            print(f"   âœ… å¯ç”¨åˆ—: {len(available_columns)}/{len(required_columns)}")
            
            # åˆ›å»º23åˆ—æ•°æ®æ¡†
            df_23col = df[available_columns].copy()
            
            # åº”ç”¨åˆ—åæ˜ å°„
            if column_mapping:
                df_23col = df_23col.rename(columns=column_mapping)
            
            # ç¡®ä¿åˆ—çš„é¡ºåºä¸è¦æ±‚ä¸€è‡´
            final_columns = [col for col in required_columns if col in df_23col.columns]
            df_23col = df_23col[final_columns]
            
            # ä¼˜åŒ–æ•°æ®ç±»å‹
            numeric_columns = df_23col.select_dtypes(include=['float64']).columns
            if len(numeric_columns) > 0:
                df_23col[numeric_columns] = df_23col[numeric_columns].astype('float32')
            
            # ç”Ÿæˆ23åˆ—æ–‡ä»¶å
            original_name = file_path.stem
            # ç§»é™¤ç°æœ‰çš„åç¼€
            if original_name.endswith('_enhanced'):
                base_name = original_name[:-9]
            elif original_name.endswith('_optimized'):
                base_name = original_name[:-10]
            elif original_name.endswith('_streamlined'):
                base_name = original_name[:-12]
            else:
                base_name = original_name
            
            col23_filename = f"{base_name}_23col.csv"
            col23_path = file_path.parent / col23_filename
            
            # ä¿å­˜23åˆ—æ–‡ä»¶
            df_23col.to_csv(col23_path, encoding='utf-8-sig', index=False)
            
            # è®¡ç®—æ–‡ä»¶å¤§å°å˜åŒ–
            original_size = file_path.stat().st_size / 1024
            new_size = col23_path.stat().st_size / 1024
            size_reduction = (original_size - new_size) / original_size * 100
            
            print(f"   âœ… 23åˆ—æ–‡ä»¶åˆ›å»ºæˆåŠŸ:")
            print(f"      æ–‡ä»¶å: {col23_filename}")
            print(f"      åˆ—æ•°: {original_cols} â†’ {len(df_23col.columns)} (å‡å°‘{original_cols - len(df_23col.columns)}åˆ—)")
            print(f"      æ–‡ä»¶å¤§å°: {original_size:.1f}KB â†’ {new_size:.1f}KB (å‡å°‘{size_reduction:.1f}%)")
            
            processed_count += 1
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    print(f"\n" + "=" * 80)
    print(f"ğŸ‰ 23åˆ—ç²¾ç®€æ–‡ä»¶åˆ›å»ºå®Œæˆ!")
    print(f"âœ… æˆåŠŸå¤„ç† {processed_count}/{len(files_to_process)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š æ¯ä¸ªæ–‡ä»¶åŒ…å«23ä¸ªæ ¸å¿ƒæŒ‡æ ‡")

def get_alternative_column_names(required_col, available_cols):
    """è·å–å¯èƒ½çš„æ›¿ä»£åˆ—å"""
    alternatives = []
    
    # å®šä¹‰åˆ—åæ˜ å°„å…³ç³»
    column_alternatives = {
        'MA89': ['MA_LONG', 'MA_89'],
        'MA_LONG': ['MA89', 'MA_89'],
        'BB_Long_Upper': ['BB_Long_Upper', 'BB_LONG_UPPER'],
        'BB_Long_Lower': ['BB_Long_Lower', 'BB_LONG_LOWER'],
        'MACD_Long': ['MACD_LONG', 'MACD_Long'],
        'Fib_Support_Level': ['Fib_Support_Level', 'FIB_SUPPORT_LEVEL'],
        'Fib_Resistance_Level': ['Fib_Resistance_Level', 'FIB_RESISTANCE_LEVEL'],
        'Fib_Price_Position': ['Fib_Price_Position', 'FIB_PRICE_POSITION']
    }
    
    if required_col in column_alternatives:
        for alt_name in column_alternatives[required_col]:
            if alt_name in available_cols:
                alternatives.append(alt_name)
    
    return alternatives

def verify_23_column_files():
    """éªŒè¯23åˆ—æ–‡ä»¶"""
    print(f"\nğŸ” éªŒè¯23åˆ—æ–‡ä»¶")
    print("=" * 80)
    
    # æŸ¥æ‰¾23åˆ—æ–‡ä»¶
    col23_files = list(DATA_DIR.glob("*_23col.csv"))
    
    if not col23_files:
        print("âŒ æœªæ‰¾åˆ°23åˆ—æ–‡ä»¶")
        return
    
    for file_path in col23_files:
        print(f"\nğŸ“Š éªŒè¯æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            
            print(f"   ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
            print(f"      æ•°æ®è¡Œæ•°: {len(df)}")
            print(f"      åˆ—æ•°: {len(df.columns)}")
            print(f"      æ–‡ä»¶å¤§å°: {file_path.stat().st_size / 1024:.1f}KB")
            
            print(f"   ğŸ“‹ åŒ…å«çš„åˆ—:")
            for i, col in enumerate(df.columns, 1):
                print(f"      {i:2d}. {col}")
            
            # æ£€æŸ¥å…³é”®æŒ‡æ ‡çš„æœ‰æ•ˆæ•°æ®
            key_indicators = ['æ”¶ç›˜ä»·', 'MA20', 'MA50', 'RSI', 'ATR']
            print(f"   ğŸ” å…³é”®æŒ‡æ ‡æ•°æ®è´¨é‡:")
            for indicator in key_indicators:
                if indicator in df.columns:
                    valid_count = df[indicator].notna().sum()
                    if valid_count > 0:
                        latest_value = df[indicator].iloc[-1]
                        print(f"      {indicator}: {valid_count}/{len(df)} æœ‰æ•ˆ, æœ€æ–°: {latest_value:.2f}")
            
            # æ£€æŸ¥æ–æ³¢é‚£å¥‘æŒ‡æ ‡
            fib_indicators = ['Fib_Ret_0.382', 'Fib_Ret_0.500', 'Fib_Ret_0.618']
            print(f"   ğŸ”¢ æ–æ³¢é‚£å¥‘æŒ‡æ ‡:")
            for fib in fib_indicators:
                if fib in df.columns:
                    valid_count = df[fib].notna().sum()
                    if valid_count > 0:
                        latest_value = df[fib].iloc[-1]
                        print(f"      {fib}: {valid_count}/{len(df)} æœ‰æ•ˆ, æœ€æ–°: ${latest_value:.2f}")
                    else:
                        print(f"      {fib}: æ— æœ‰æ•ˆæ•°æ®")
            
        except Exception as e:
            print(f"   âŒ éªŒè¯å¤±è´¥: {e}")

def show_23_column_usage_guide():
    """æ˜¾ç¤º23åˆ—æ–‡ä»¶ä½¿ç”¨æŒ‡å—"""
    print(f"\nğŸ’¡ 23åˆ—ç²¾ç®€æ–‡ä»¶ä½¿ç”¨æŒ‡å—")
    print("=" * 80)
    
    print("ğŸ¯ 23åˆ—æ–‡ä»¶ç‰¹ç‚¹:")
    print("   â€¢ æ ¸å¿ƒæŒ‡æ ‡: åŒ…å«æœ€é‡è¦çš„23ä¸ªæŠ€æœ¯æŒ‡æ ‡")
    print("   â€¢ æ–‡ä»¶ç²¾ç®€: å¤§å¹…å‡å°‘æ–‡ä»¶å¤§å°ï¼Œä¼ è¾“æ›´å¿«")
    print("   â€¢ AIå‹å¥½: ä¸“ä¸ºDeepSeek AIåˆ†æä¼˜åŒ–")
    print("   â€¢ æ•°æ®å®Œæ•´: ä¿ç•™æ‰€æœ‰å…³é”®çš„äº¤æ˜“ä¿¡å·")
    
    print(f"\nğŸ“Š æŒ‡æ ‡åˆ†ç±»:")
    categories = {
        'åŸºç¡€æ•°æ® (6åˆ—)': ['open_time', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é‡'],
        'ç§»åŠ¨å¹³å‡ (3åˆ—)': ['MA20', 'MA50', 'MA89'],
        'å¸ƒæ—å¸¦ (4åˆ—)': ['BB_Upper', 'BB_Lower', 'BB_Long_Upper', 'BB_Long_Lower'],
        'åŠ¨é‡æŒ‡æ ‡ (3åˆ—)': ['MACD_Hist', 'MACD_Long', 'RSI'],
        'æ³¢åŠ¨ç‡ (1åˆ—)': ['ATR'],
        'æ–æ³¢é‚£å¥‘ (6åˆ—)': ['Fib_Ret_0.382', 'Fib_Ret_0.500', 'Fib_Ret_0.618', 
                        'Fib_Support_Level', 'Fib_Resistance_Level', 'Fib_Price_Position']
    }
    
    for category, indicators in categories.items():
        print(f"   â€¢ {category}: {', '.join(indicators)}")
    
    print(f"\nğŸ¤– DeepSeek AIä½¿ç”¨å»ºè®®:")
    print("   1. ä¸Šä¼ 23åˆ—æ–‡ä»¶è¿›è¡Œå¿«é€Ÿåˆ†æ")
    print("   2. é‡ç‚¹å…³æ³¨MAç³»ç»Ÿçš„è¶‹åŠ¿æ–¹å‘")
    print("   3. ç»“åˆå¸ƒæ—å¸¦åˆ¤æ–­ä»·æ ¼é€šé“")
    print("   4. ä½¿ç”¨æ–æ³¢é‚£å¥‘æ°´å¹³ç¡®å®šæ”¯æ’‘é˜»åŠ›")
    print("   5. RSIå’ŒMACDæä¾›åŠ¨é‡ç¡®è®¤")

def main():
    """ä¸»å‡½æ•°"""
    print("BTCUSDT 23åˆ—ç²¾ç®€æ•°æ®æ–‡ä»¶ç”Ÿæˆå™¨")
    print("=" * 80)
    print("ç›®æ ‡: åˆ›å»ºåªåŒ…å«ç”¨æˆ·æŒ‡å®š23åˆ—çš„ç²¾ç®€æ•°æ®æ–‡ä»¶")
    print("=" * 80)
    
    # 1. åˆ›å»º23åˆ—æ–‡ä»¶
    create_23_column_files()
    
    # 2. éªŒè¯23åˆ—æ–‡ä»¶
    verify_23_column_files()
    
    # 3. æ˜¾ç¤ºä½¿ç”¨æŒ‡å—
    show_23_column_usage_guide()
    
    print(f"\n" + "=" * 80)
    print("ğŸ‰ 23åˆ—ç²¾ç®€æ•°æ®æ–‡ä»¶ç”Ÿæˆå®Œæˆ!")
    print("âœ… åŒ…å«ç”¨æˆ·æŒ‡å®šçš„23ä¸ªæ ¸å¿ƒæŒ‡æ ‡")
    print("âœ… æ–‡ä»¶å¤§å°å¤§å¹…å‡å°‘")
    print("âœ… ä¿ç•™æ‰€æœ‰å…³é”®äº¤æ˜“ä¿¡å·")
    print("âœ… é€‚åˆDeepSeek AIå¿«é€Ÿåˆ†æ")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„23åˆ—æ–‡ä»¶:")
    col23_files = list(DATA_DIR.glob("*_23col.csv"))
    for file in col23_files:
        size_kb = file.stat().st_size / 1024
        print(f"   â€¢ {file.name} ({size_kb:.1f}KB)")

if __name__ == "__main__":
    main()
