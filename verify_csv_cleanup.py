"""
CSVæ¸…ç†éªŒè¯è„šæœ¬
éªŒè¯BB_Squeezeå’Œå…¶ä»–å¤šä½™æ•°æ®æ˜¯å¦å·²æˆåŠŸç§»é™¤
"""

import pandas as pd
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from config import DATA_DIR

def verify_csv_cleanup():
    """éªŒè¯CSVæ¸…ç†ç»“æœ"""
    print("ğŸ” CSVæ¸…ç†ç»“æœéªŒè¯")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»„åˆæ•°æ®æ–‡ä»¶
    csv_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    backup_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.backup.csv"))
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°ç»„åˆæ•°æ®CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªç»„åˆæ•°æ®æ–‡ä»¶")
    print(f"ğŸ’¾ æ‰¾åˆ° {len(backup_files)} ä¸ªå¤‡ä»½æ–‡ä»¶")
    
    # å®šä¹‰åº”è¯¥è¢«ç§»é™¤çš„åˆ—
    unwanted_columns = [
        'BB_Squeeze',           # å¸ƒæ—å¸¦æŒ¤å‹æ ‡å¿—
        'BB_Width',             # å¸ƒæ—å¸¦å®½åº¦
        'MA8', 'MA21', 'MA55',  # é‡å¤çš„MAåˆ—
        'MACD_Long_Hist',       # é•¿æœŸMACDæŸ±çŠ¶å›¾
        'RSI_Extra_Long',       # è¶…é•¿æœŸRSI
        'è®¡ç®—æ—¶é—´',              # è®¡ç®—æ—¶é—´æˆ³
        'MA_Signal',            # MAä¿¡å·
        'MACD_Signal_Analysis', # MACDä¿¡å·åˆ†æ
        'RSI_Signal',           # RSIä¿¡å·
        'BB_Signal',            # BBä¿¡å·
        'Stoch_Signal',         # éšæœºæŒ‡æ ‡ä¿¡å·
        'ç»¼åˆä¿¡å·'              # ç»¼åˆä¿¡å·
    ]
    
    # å®šä¹‰æ ¸å¿ƒå¿…é¡»ä¿ç•™çš„åˆ—
    required_columns = [
        'open_time',            # æ—¶é—´
        'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·',  # OHLC
        'æˆäº¤é‡',               # æˆäº¤é‡
        'MA20', 'MA50',         # åŸºç¡€ç§»åŠ¨å¹³å‡çº¿
        'MACD', 'MACD_Signal', 'MACD_Hist',  # MACD
        'RSI',                  # RSI
        'BB_Upper', 'BB_Middle', 'BB_Lower',  # å¸ƒæ—å¸¦
        'ATR',                  # ATR
        'ADX'                   # ADX
    ]
    
    all_passed = True
    
    for csv_file in csv_files:
        if csv_file.name.endswith('.backup.csv'):
            continue
            
        print(f"\nğŸ“Š éªŒè¯æ–‡ä»¶: {csv_file.name}")
        print("-" * 60)
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            
            # æ£€æŸ¥æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
            print(f"   æ•°æ®ç»´åº¦: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            print(f"   å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸éœ€è¦çš„åˆ—
            found_unwanted = [col for col in unwanted_columns if col in df.columns]
            if found_unwanted:
                print(f"   âŒ ä»åŒ…å«å¤šä½™åˆ—: {found_unwanted}")
                all_passed = False
            else:
                print(f"   âœ… å¤šä½™åˆ—å·²æ¸…ç†å®Œæˆ")
            
            # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
            missing_required = [col for col in required_columns if col not in df.columns]
            if missing_required:
                print(f"   âš ï¸ ç¼ºå°‘å¿…éœ€åˆ—: {missing_required}")
                all_passed = False
            else:
                print(f"   âœ… æ ¸å¿ƒåˆ—å®Œæ•´")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹ä¼˜åŒ–
            float64_cols = df.select_dtypes(include=['float64']).columns
            if len(float64_cols) > 0:
                print(f"   âš ï¸ ä»æœ‰{len(float64_cols)}åˆ—ä½¿ç”¨float64 (å¯è¿›ä¸€æ­¥ä¼˜åŒ–)")
            else:
                print(f"   âœ… æ•°æ®ç±»å‹å·²ä¼˜åŒ–")
            
            # æ£€æŸ¥åˆ—é¡ºåº
            first_cols = list(df.columns[:10])
            expected_first_cols = ['open_time', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æˆäº¤ç¬”æ•°', 'ä¸»åŠ¨ä¹°å…¥é‡', 'ä¸»åŠ¨ä¹°å…¥é¢']
            if first_cols == expected_first_cols:
                print(f"   âœ… åˆ—é¡ºåºå·²ä¼˜åŒ–")
            else:
                print(f"   âš ï¸ åˆ—é¡ºåºå¯èƒ½éœ€è¦è°ƒæ•´")
            
            # æ˜¾ç¤ºå½“å‰åˆ—ç»“æ„
            print(f"   ğŸ“‹ å½“å‰åˆ—ç»“æ„:")
            
            # æŒ‰ç±»åˆ«æ˜¾ç¤ºåˆ—
            basic_cols = [col for col in df.columns if col in ['open_time', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æˆäº¤ç¬”æ•°', 'ä¸»åŠ¨ä¹°å…¥é‡', 'ä¸»åŠ¨ä¹°å…¥é¢']]
            ma_cols = [col for col in df.columns if 'MA' in col and col not in ['MACD', 'MACD_Signal', 'MACD_Hist']]
            macd_cols = [col for col in df.columns if 'MACD' in col]
            rsi_cols = [col for col in df.columns if 'RSI' in col]
            bb_cols = [col for col in df.columns if 'BB_' in col]
            other_cols = [col for col in df.columns if col not in basic_cols + ma_cols + macd_cols + rsi_cols + bb_cols]
            
            print(f"      åŸºç¡€æ•°æ® ({len(basic_cols)}): {basic_cols}")
            print(f"      ç§»åŠ¨å¹³å‡ ({len(ma_cols)}): {ma_cols}")
            print(f"      MACD ({len(macd_cols)}): {macd_cols}")
            print(f"      RSI ({len(rsi_cols)}): {rsi_cols}")
            print(f"      å¸ƒæ—å¸¦ ({len(bb_cols)}): {bb_cols}")
            print(f"      å…¶ä»–æŒ‡æ ‡ ({len(other_cols)}): {other_cols}")
            
        except Exception as e:
            print(f"   âŒ éªŒè¯å¤±è´¥: {e}")
            all_passed = False
    
    # æ€»ç»“
    print(f"\n" + "=" * 80)
    if all_passed:
        print("âœ… æ‰€æœ‰æ–‡ä»¶æ¸…ç†éªŒè¯é€šè¿‡!")
    else:
        print("âš ï¸ éƒ¨åˆ†æ–‡ä»¶éœ€è¦è¿›ä¸€æ­¥å¤„ç†")
    
    return all_passed

def compare_before_after():
    """å¯¹æ¯”æ¸…ç†å‰åçš„å·®å¼‚"""
    print(f"\nğŸ“Š æ¸…ç†å‰åå¯¹æ¯”")
    print("=" * 80)
    
    csv_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    
    total_space_saved = 0
    total_columns_removed = 0
    
    for csv_file in csv_files:
        if csv_file.name.endswith('.backup.csv'):
            continue
            
        backup_file = csv_file.with_suffix('.backup.csv')
        
        if backup_file.exists():
            try:
                df_before = pd.read_csv(backup_file, encoding='utf-8-sig')
                df_after = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                size_before = backup_file.stat().st_size / 1024  # KB
                size_after = csv_file.stat().st_size / 1024     # KB
                space_saved = size_before - size_after
                
                columns_removed = len(df_before.columns) - len(df_after.columns)
                
                print(f"\nğŸ“ {csv_file.name}:")
                print(f"   åˆ—æ•°: {len(df_before.columns)} â†’ {len(df_after.columns)} (å‡å°‘{columns_removed}åˆ—)")
                print(f"   æ–‡ä»¶å¤§å°: {size_before:.1f}KB â†’ {size_after:.1f}KB (èŠ‚çœ{space_saved:.1f}KB)")
                print(f"   ç©ºé—´èŠ‚çœ: {space_saved/size_before*100:.1f}%")
                
                # æ˜¾ç¤ºè¢«ç§»é™¤çš„åˆ—
                removed_cols = set(df_before.columns) - set(df_after.columns)
                if removed_cols:
                    print(f"   ç§»é™¤çš„åˆ—: {list(removed_cols)}")
                
                total_space_saved += space_saved
                total_columns_removed += columns_removed
                
            except Exception as e:
                print(f"âŒ å¯¹æ¯”å¤±è´¥: {e}")
    
    print(f"\nğŸ“ˆ æ€»è®¡:")
    print(f"   æ€»å…±ç§»é™¤åˆ—æ•°: {total_columns_removed}")
    print(f"   æ€»å…±èŠ‚çœç©ºé—´: {total_space_saved:.1f}KB")

def cleanup_recommendations():
    """æä¾›æ¸…ç†å»ºè®®"""
    print(f"\nğŸ’¡ æ¸…ç†å»ºè®®")
    print("=" * 80)
    
    print("âœ… å·²å®Œæˆçš„æ¸…ç†:")
    print("   - ç§»é™¤äº† BB_Squeeze åˆ— (å¸ƒæ—å¸¦æŒ¤å‹æ ‡å¿—)")
    print("   - ç§»é™¤äº† BB_Width åˆ— (å¸ƒæ—å¸¦å®½åº¦ä¸­é—´æ•°æ®)")
    print("   - ç§»é™¤äº†é‡å¤çš„MAåˆ— (MA8, MA21, MA55)")
    print("   - ç§»é™¤äº†éƒ¨åˆ†é•¿æœŸæŒ‡æ ‡ (RSI_Extra_Long, MACD_Long_Hist)")
    print("   - ä¼˜åŒ–äº†æ•°æ®ç±»å‹ (float64 â†’ float32)")
    print("   - ä¼˜åŒ–äº†åˆ—é¡ºåº (æ ¸å¿ƒæŒ‡æ ‡å‰ç½®)")
    
    print("\nğŸ”§ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
    print("   1. å¦‚æœä¸éœ€è¦é•¿æœŸæŒ‡æ ‡ï¼Œå¯ä»¥ç§»é™¤:")
    print("      - RSI_Secondary, RSI_Long")
    print("      - BB_Long_Upper, BB_Long_Middle, BB_Long_Lower")
    print("      - ATR_Long, ATR_Ratio")
    print("      - MACD_Long, MACD_Long_Signal")
    
    print("\n   2. å¦‚æœåªéœ€è¦åŸºç¡€åˆ†æï¼Œå¯ä»¥ä¿ç•™æ ¸å¿ƒåˆ—:")
    print("      - åŸºç¡€æ•°æ®: open_time, OHLC, æˆäº¤é‡")
    print("      - æ ¸å¿ƒæŒ‡æ ‡: MA20, MA50, MACD, RSI, BB, ATR, ADX")
    
    print("\n   3. å¤‡ä»½æ–‡ä»¶ç®¡ç†:")
    print("      - å¦‚æœæ¸…ç†ç»“æœæ»¡æ„ï¼Œå¯ä»¥åˆ é™¤ *.backup.csv æ–‡ä»¶")
    print("      - å¤‡ä»½æ–‡ä»¶å ç”¨é¢å¤–å­˜å‚¨ç©ºé—´")
    
    print("\nğŸ“‹ å½“å‰æ¨èçš„æœ€å°æ ¸å¿ƒåˆ—é›†åˆ (24åˆ—):")
    core_columns = [
        'open_time', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é‡',
        'MA20', 'MA50', 'MACD', 'MACD_Signal', 'MACD_Hist',
        'RSI', 'BB_Upper', 'BB_Middle', 'BB_Lower',
        'Stoch_SlowK', 'Stoch_SlowD', 'OBV', 'ATR', 'ADX'
    ]
    print("   " + ", ".join(core_columns))

def main():
    """ä¸»å‡½æ•°"""
    print("BTCUSDT CSVæ¸…ç†éªŒè¯å·¥å…·")
    print("=" * 80)
    
    # éªŒè¯æ¸…ç†ç»“æœ
    verification_passed = verify_csv_cleanup()
    
    # å¯¹æ¯”æ¸…ç†å‰å
    compare_before_after()
    
    # æä¾›å»ºè®®
    cleanup_recommendations()
    
    print(f"\n" + "=" * 80)
    if verification_passed:
        print("ğŸ‰ CSVæ¸…ç†ä»»åŠ¡å®Œæˆ!")
        print("âœ… BB_Squeeze å’Œå…¶ä»–å¤šä½™æ•°æ®å·²æˆåŠŸç§»é™¤")
        print("âœ… æ–‡ä»¶ç»“æ„å·²ä¼˜åŒ–ï¼Œæ•°æ®æ›´åŠ ç²¾ç®€")
        print("âœ… æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯æŒ‡æ ‡å®Œæ•´ä¿ç•™")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸…ç†ä»»åŠ¡éœ€è¦è¿›ä¸€æ­¥å¤„ç†")

if __name__ == "__main__":
    main()
