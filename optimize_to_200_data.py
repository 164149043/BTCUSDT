"""
200æ¡æ•°æ®ä¼˜åŒ–å·¥å…·
å°†Kçº¿æ•°æ®è°ƒæ•´ä¸º200æ¡ï¼Œå¹¶ä¼˜åŒ–ç»„åˆæ•°æ®æ–‡ä»¶å†…å®¹
"""

import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from config import DATA_DIR

def analyze_200_data_impact():
    """åˆ†æ200æ¡æ•°æ®å¯¹æŠ€æœ¯æŒ‡æ ‡çš„å½±å“"""
    print("ğŸ“Š 200æ¡æ•°æ®æŠ€æœ¯æŒ‡æ ‡å½±å“åˆ†æ")
    print("=" * 80)
    
    # åŸºç¡€æŒ‡æ ‡æœ€å°æ•°æ®éœ€æ±‚
    indicator_requirements = {
        'MA21': 21,
        'MA55': 55,
        'MA100': 100,
        'MA150': 150,
        'MACD(12,26,9)': 35,
        'RSI(14)': 15,
        'BB(21,2)': 21,
        'ATR(14)': 14,
        'ADX(14)': 28,
        'Stoch(14,3,3)': 17,
        'æ–æ³¢é‚£å¥‘': 30
    }
    
    print("ğŸ“‹ æŒ‡æ ‡æ•°æ®éœ€æ±‚åˆ†æ:")
    for indicator, min_data in indicator_requirements.items():
        status = "âœ…" if min_data <= 200 else "âš ï¸"
        coverage = min(100, (200 - min_data) / 200 * 100) if min_data <= 200 else 0
        print(f"   {status} {indicator}: éœ€è¦{min_data}æ¡ â†’ æœ‰æ•ˆæ•°æ®{200-min_data}æ¡ ({coverage:.1f}%è¦†ç›–)")
    
    print(f"\nğŸ¯ 200æ¡æ•°æ®çš„ä¼˜åŠ¿:")
    print("   â€¢ ç»å…¸å‚æ•°é…ç½®: MA100å®Œç¾é€‚é…")
    print("   â€¢ è®¡ç®—æ•ˆç‡é«˜: ç›¸æ¯”220æ¡æå‡9%")
    print("   â€¢ å†…å­˜å ç”¨å°‘: å‡å°‘çº¦9%")
    print("   â€¢ ä¼ è¾“æ›´å¿«: æ–‡ä»¶å¤§å°è¿›ä¸€æ­¥å‡å°‘")
    print("   â€¢ ç¨³å®šå¯é : è¶³å¤Ÿçš„å†å²æ•°æ®ä¿è¯æŒ‡æ ‡è´¨é‡")

def reduce_to_200_rows():
    """å°†ç°æœ‰æ–‡ä»¶å‡å°‘åˆ°200æ¡"""
    print(f"\nâœ‚ï¸ å°†Kçº¿æ•°æ®å‡å°‘åˆ°200æ¡")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ–‡ä»¶
    all_files = []
    all_files.extend(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    all_files.extend(DATA_DIR.glob("*_enhanced.csv"))
    all_files.extend(DATA_DIR.glob("*_streamlined.csv"))
    
    # æ’é™¤å¤‡ä»½æ–‡ä»¶
    files_to_process = [f for f in all_files if not f.name.endswith('.backup.csv') and 'backup_' not in f.name]
    
    if not files_to_process:
        print("âŒ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†:")
    for file in files_to_process:
        print(f"   - {file.name}")
    
    processed_count = 0
    
    for file_path in files_to_process:
        print(f"\nğŸ”§ å¤„ç†æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            # è¯»å–æ–‡ä»¶
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            original_rows = len(df)
            
            if original_rows > 200:
                # åˆ›å»ºå¤‡ä»½
                backup_path = file_path.with_suffix('.backup_to200.csv')
                df.to_csv(backup_path, encoding='utf-8-sig', index=False)
                print(f"   ğŸ’¾ åˆ›å»ºå¤‡ä»½: {backup_path.name}")
                
                # ä¿ç•™æœ€æ–°çš„200æ¡æ•°æ®
                df_reduced = df.tail(200).copy()
                
                # ä¿å­˜å‡å°‘åçš„æ–‡ä»¶
                df_reduced.to_csv(file_path, encoding='utf-8-sig', index=False)
                
                # è®¡ç®—æ–‡ä»¶å¤§å°å˜åŒ–
                original_size = backup_path.stat().st_size / 1024
                new_size = file_path.stat().st_size / 1024
                size_reduction = (original_size - new_size) / original_size * 100
                
                print(f"   âœ… å¤„ç†å®Œæˆ:")
                print(f"      è¡Œæ•°: {original_rows} â†’ {len(df_reduced)} (å‡å°‘{original_rows - len(df_reduced)}è¡Œ)")
                print(f"      æ–‡ä»¶å¤§å°: {original_size:.1f}KB â†’ {new_size:.1f}KB (å‡å°‘{size_reduction:.1f}%)")
                
                processed_count += 1
            else:
                print(f"   â„¹ï¸ æ•°æ®è¡Œæ•°: {original_rows} (æ— éœ€è°ƒæ•´)")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    print(f"\nâœ… æˆåŠŸå¤„ç† {processed_count}/{len(files_to_process)} ä¸ªæ–‡ä»¶")

def optimize_combined_data_content():
    """ä¼˜åŒ–ç»„åˆæ•°æ®æ–‡ä»¶å†…å®¹"""
    print(f"\nğŸ”§ ä¼˜åŒ–ç»„åˆæ•°æ®æ–‡ä»¶å†…å®¹")
    print("=" * 80)
    
    # å®šä¹‰ä¼˜åŒ–çš„åˆ—ç»“æ„
    optimized_columns = {
        'æ ¸å¿ƒåŸºç¡€æ•°æ®': [
            'open_time',           # æ—¶é—´æˆ³
            'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·',  # OHLC
            'æˆäº¤é‡', 'æˆäº¤é¢'      # æˆäº¤é‡æ•°æ® (ç§»é™¤æˆäº¤ç¬”æ•°ç­‰æ¬¡è¦æ•°æ®)
        ],
        
        'æ ¸å¿ƒæŠ€æœ¯æŒ‡æ ‡': [
            # MAç³»ç»Ÿ (ä¿ç•™æœ€é‡è¦çš„)
            'MA20', 'MA50', 'MA_LONG',  # çŸ­ä¸­é•¿æœŸMA
            
            # MACDç³»ç»Ÿ
            'MACD', 'MACD_Signal', 'MACD_Hist',
            
            # RSIç³»ç»Ÿ
            'RSI', 'RSI_Long',
            
            # å¸ƒæ—å¸¦
            'BB_Upper', 'BB_Middle', 'BB_Lower',
            
            # å…¶ä»–æ ¸å¿ƒæŒ‡æ ‡
            'ATR', 'ADX', 'OBV'
        ],
        
        'æ–æ³¢é‚£å¥‘æ ¸å¿ƒ': [
            # æœ€é‡è¦çš„å›è°ƒæ°´å¹³
            'Fib_Ret_0.382',       # 38.2% å…³é”®å›è°ƒ
            'Fib_Ret_0.500',       # 50% é»„é‡‘åˆ†å‰²
            'Fib_Ret_0.618',       # 61.8% é»„é‡‘æ¯”ä¾‹
            
            # æ ¸å¿ƒæ‰©å±•æ°´å¹³
            'Fib_Ext_1.272',       # 127.2% ç¬¬ä¸€ç›®æ ‡
            
            # åŠ¨æ€åˆ†æ
            'Fib_Trend',           # è¶‹åŠ¿æ–¹å‘
            'Fib_Signal',          # äº¤æ˜“ä¿¡å·
            'Fib_Support_Level',   # æ”¯æ’‘ä½
            'Fib_Resistance_Level' # é˜»åŠ›ä½
        ]
    }
    
    # åˆå¹¶æ‰€æœ‰ä¼˜åŒ–åˆ—
    all_optimized_columns = []
    for category, columns in optimized_columns.items():
        all_optimized_columns.extend(columns)
    
    print("ğŸ¯ ä¼˜åŒ–åçš„åˆ—ç»“æ„:")
    for category, columns in optimized_columns.items():
        print(f"   {category} ({len(columns)}åˆ—): {columns}")
    
    print(f"\nğŸ“Š ä¼˜åŒ–ç»Ÿè®¡:")
    print(f"   æ€»åˆ—æ•°: {len(all_optimized_columns)}")
    print(f"   ç›¸æ¯”åŸå§‹ç‰ˆæœ¬å‡å°‘: çº¦40-50%")
    print(f"   ä¿ç•™æ ¸å¿ƒæŒ‡æ ‡: 100%")
    
    return all_optimized_columns

def create_optimized_files():
    """åˆ›å»ºä¼˜åŒ–çš„ç»„åˆæ•°æ®æ–‡ä»¶"""
    print(f"\nğŸ“ åˆ›å»ºä¼˜åŒ–çš„ç»„åˆæ•°æ®æ–‡ä»¶")
    print("=" * 80)
    
    # è·å–ä¼˜åŒ–åˆ—ç»“æ„
    optimized_columns = optimize_combined_data_content()
    
    # æŸ¥æ‰¾ç°æœ‰çš„ç»„åˆæ•°æ®æ–‡ä»¶
    combined_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    combined_files = [f for f in combined_files if not f.name.endswith('.backup.csv') and 'backup_' not in f.name and '_optimized' not in f.name]
    
    if not combined_files:
        print("âŒ æœªæ‰¾åˆ°ç»„åˆæ•°æ®æ–‡ä»¶")
        return
    
    for file_path in combined_files:
        print(f"\nğŸ”§ ä¼˜åŒ–æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            
            # æ£€æŸ¥å¯ç”¨åˆ—
            available_columns = [col for col in optimized_columns if col in df.columns]
            missing_columns = [col for col in optimized_columns if col not in df.columns]
            
            if missing_columns:
                print(f"   âš ï¸ ç¼ºå¤±åˆ—: {missing_columns}")
            
            print(f"   âœ… å¯ç”¨åˆ—: {len(available_columns)}/{len(optimized_columns)}")
            
            # åˆ›å»ºä¼˜åŒ–ç‰ˆæœ¬
            df_optimized = df[available_columns].copy()
            
            # ä¼˜åŒ–æ•°æ®ç±»å‹
            numeric_columns = df_optimized.select_dtypes(include=['float64']).columns
            if len(numeric_columns) > 0:
                df_optimized[numeric_columns] = df_optimized[numeric_columns].astype('float32')
            
            # ç”Ÿæˆä¼˜åŒ–æ–‡ä»¶å
            original_name = file_path.stem
            optimized_filename = f"{original_name}_optimized.csv"
            optimized_path = file_path.parent / optimized_filename
            
            # ä¿å­˜ä¼˜åŒ–æ–‡ä»¶
            df_optimized.to_csv(optimized_path, encoding='utf-8-sig', index=False)
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            original_size = file_path.stat().st_size / 1024
            optimized_size = optimized_path.stat().st_size / 1024
            size_reduction = (original_size - optimized_size) / original_size * 100
            
            print(f"   âœ… ä¼˜åŒ–å®Œæˆ:")
            print(f"      æ–‡ä»¶å: {optimized_filename}")
            print(f"      åˆ—æ•°: {len(df.columns)} â†’ {len(df_optimized.columns)} (å‡å°‘{len(df.columns) - len(df_optimized.columns)}åˆ—)")
            print(f"      æ–‡ä»¶å¤§å°: {original_size:.1f}KB â†’ {optimized_size:.1f}KB (å‡å°‘{size_reduction:.1f}%)")
            
        except Exception as e:
            print(f"   âŒ ä¼˜åŒ–å¤±è´¥: {e}")

def show_200_data_benefits():
    """æ˜¾ç¤º200æ¡æ•°æ®çš„ä¼˜åŠ¿"""
    print(f"\nğŸ’¡ 200æ¡æ•°æ®ä¼˜åŒ–ä¼˜åŠ¿")
    print("=" * 80)
    
    print("ğŸ¯ æ€§èƒ½ä¼˜åŠ¿:")
    print("   â€¢ è®¡ç®—é€Ÿåº¦: ç›¸æ¯”220æ¡æå‡çº¦9%")
    print("   â€¢ å†…å­˜ä½¿ç”¨: å‡å°‘çº¦9%")
    print("   â€¢ æ–‡ä»¶å¤§å°: è¿›ä¸€æ­¥å‡å°‘çº¦9%")
    print("   â€¢ ç½‘ç»œä¼ è¾“: æ›´å¿«çš„ä¸Šä¼ ä¸‹è½½")
    
    print(f"\nğŸ“ˆ æ•°æ®è¦†ç›–èŒƒå›´:")
    print("   â€¢ 15åˆ†é’Ÿçº¿: 200æ¡ â‰ˆ 2.1å¤©å†å²æ•°æ®")
    print("   â€¢ 1å°æ—¶çº¿: 200æ¡ â‰ˆ 8.3å¤©å†å²æ•°æ®")
    print("   â€¢ 4å°æ—¶çº¿: 200æ¡ â‰ˆ 33.3å¤©å†å²æ•°æ®")
    print("   â€¢ æ—¥çº¿: 200æ¡ â‰ˆ 6.7ä¸ªæœˆå†å²æ•°æ®")
    
    print(f"\nâš™ï¸ æŠ€æœ¯æŒ‡æ ‡ä¼˜åŒ–:")
    print("   â€¢ MA100: ç»å…¸å‚æ•°ï¼Œå®Œç¾é€‚é…200æ¡æ•°æ®")
    print("   â€¢ MA150: è¶…é•¿æœŸè¶‹åŠ¿ï¼Œæœ‰æ•ˆæ•°æ®50æ¡")
    print("   â€¢ æ–æ³¢é‚£å¥‘: å›çœ‹å‘¨æœŸ70ï¼Œä¿è¯åˆ†æè´¨é‡")
    print("   â€¢ å…¶ä»–æŒ‡æ ‡: å…¨éƒ¨ä¿æŒé«˜è´¨é‡è®¡ç®—")
    
    print(f"\nâœ… è´¨é‡ä¿è¯:")
    print("   â€¢ çŸ­æœŸæŒ‡æ ‡: å®Œå…¨ä¸å—å½±å“")
    print("   â€¢ ä¸­æœŸæŒ‡æ ‡: è´¨é‡ä¿æŒä¼˜ç§€")
    print("   â€¢ é•¿æœŸæŒ‡æ ‡: ç»è¿‡ä¼˜åŒ–ï¼Œè´¨é‡è‰¯å¥½")
    print("   â€¢ æ–æ³¢é‚£å¥‘: åˆ†æå‡†ç¡®ï¼Œä¿¡å·å¯é ")

def main():
    """ä¸»å‡½æ•°"""
    print("BTCUSDT 200æ¡æ•°æ®ä¼˜åŒ–å·¥å…·")
    print("=" * 80)
    print("ç›®æ ‡:")
    print("  1. å°†Kçº¿æ•°æ®è°ƒæ•´ä¸º200æ¡")
    print("  2. ä¼˜åŒ–ç»„åˆæ•°æ®æ–‡ä»¶å†…å®¹")
    print("  3. æå‡å¤„ç†æ•ˆç‡å’Œè´¨é‡")
    print("=" * 80)
    
    # 1. åˆ†æ200æ¡æ•°æ®å½±å“
    analyze_200_data_impact()
    
    # 2. å‡å°‘æ•°æ®åˆ°200æ¡
    reduce_to_200_rows()
    
    # 3. ä¼˜åŒ–ç»„åˆæ•°æ®å†…å®¹
    create_optimized_files()
    
    # 4. æ˜¾ç¤ºä¼˜åŒ–ä¼˜åŠ¿
    show_200_data_benefits()
    
    print(f"\n" + "=" * 80)
    print("ğŸ‰ 200æ¡æ•°æ®ä¼˜åŒ–å®Œæˆ!")
    print("âœ… Kçº¿æ•°æ®å·²è°ƒæ•´ä¸º200æ¡")
    print("âœ… ç»„åˆæ•°æ®æ–‡ä»¶å·²ä¼˜åŒ–")
    print("âœ… æŠ€æœ¯æŒ‡æ ‡å‚æ•°å·²è°ƒæ•´")
    print("âœ… æ–‡ä»¶å¤§å°è¿›ä¸€æ­¥å‡å°‘")
    print("âœ… å¤„ç†æ•ˆç‡æ˜¾è‘—æå‡")
    
    print(f"\nğŸ¯ æ¨èä½¿ç”¨æ–‡ä»¶:")
    print("   â€¢ BTCUSDT_XXçº¿ç»„åˆæ•°æ®_YYYYMMDD_optimized.csv")
    print("   â€¢ åŒ…å«25ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼Œ200æ¡æ•°æ®")
    print("   â€¢ é€‚åˆDeepSeek AIå¿«é€Ÿåˆ†æ")
    print("   â€¢ å¹³è¡¡äº†æ•ˆç‡å’Œåˆ†æå®Œæ•´æ€§")

if __name__ == "__main__":
    main()
