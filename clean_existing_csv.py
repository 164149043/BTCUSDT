"""
æ¸…ç†ç°æœ‰CSVæ–‡ä»¶è„šæœ¬
ç§»é™¤BB_Squeezeå’Œå…¶ä»–å¤šä½™æ•°æ®ï¼Œä¼˜åŒ–ç»„åˆæ•°æ®æ–‡ä»¶
"""

import pandas as pd
import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from config import DATA_DIR

def analyze_csv_columns(file_path):
    """åˆ†æCSVæ–‡ä»¶çš„åˆ—ç»“æ„"""
    print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {file_path.name}")
    print("=" * 60)
    
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        print(f"æ•°æ®ç»´åº¦: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        print(f"æ—¶é—´èŒƒå›´: {df.iloc[0, 0]} è‡³ {df.iloc[-1, 0]}")
        
        # åˆ†ç±»æ˜¾ç¤ºåˆ—å
        basic_columns = []
        ma_columns = []
        indicator_columns = []
        redundant_columns = []
        
        for col in df.columns:
            if col in ['open_time', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æˆäº¤ç¬”æ•°', 'ä¸»åŠ¨ä¹°å…¥é‡', 'ä¸»åŠ¨ä¹°å…¥é¢']:
                basic_columns.append(col)
            elif 'MA' in col:
                ma_columns.append(col)
            elif col in ['BB_Squeeze', 'BB_Width', 'MA8', 'MA21', 'MA55']:
                redundant_columns.append(col)
            else:
                indicator_columns.append(col)
        
        print(f"\nğŸ“ˆ åŸºç¡€æ•°æ®åˆ— ({len(basic_columns)}): {basic_columns}")
        print(f"ğŸ“Š ç§»åŠ¨å¹³å‡çº¿åˆ— ({len(ma_columns)}): {ma_columns}")
        print(f"ğŸ”§ æŠ€æœ¯æŒ‡æ ‡åˆ— ({len(indicator_columns)}): {indicator_columns}")
        print(f"ğŸ—‘ï¸ å¤šä½™æ•°æ®åˆ— ({len(redundant_columns)}): {redundant_columns}")
        
        return df, redundant_columns
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None, []

def clean_csv_file(file_path, backup=True):
    """æ¸…ç†å•ä¸ªCSVæ–‡ä»¶"""
    print(f"\nğŸ§¹ æ¸…ç†æ–‡ä»¶: {file_path.name}")
    print("=" * 60)
    
    try:
        # è¯»å–åŸå§‹æ•°æ®
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        original_columns = len(df.columns)
        
        # å®šä¹‰è¦ç§»é™¤çš„åˆ—
        columns_to_remove = [
            # ä¸­é—´è®¡ç®—æ•°æ®
            'BB_Squeeze',           # å¸ƒæ—å¸¦æŒ¤å‹æ ‡å¿—
            'BB_Width',             # å¸ƒæ—å¸¦å®½åº¦
            
            # é‡å¤çš„MAåˆ— (ä¿ç•™æ ‡å‡†å‘½åMA20, MA50, MA_LONG)
            'MA8', 'MA21', 'MA55',  # ç§»é™¤åŠ¨æ€å‘½åçš„MA
            
            # å…¶ä»–å¯èƒ½çš„å¤šä½™åˆ—
            'MACD_Long_Hist',       # é•¿æœŸMACDæŸ±çŠ¶å›¾
            'RSI_Extra_Long',       # è¶…é•¿æœŸRSI
            
            # ä¿¡å·åˆ†æåˆ— (å¦‚æœå­˜åœ¨)
            'è®¡ç®—æ—¶é—´',
            'MA_Signal',
            'MACD_Signal_Analysis',
            'RSI_Signal', 
            'BB_Signal',
            'Stoch_Signal',
            'ç»¼åˆä¿¡å·'
        ]
        
        # æ£€æŸ¥å“ªäº›åˆ—å®é™…å­˜åœ¨
        existing_columns_to_remove = [col for col in columns_to_remove if col in df.columns]
        
        if existing_columns_to_remove:
            # åˆ›å»ºå¤‡ä»½
            if backup:
                backup_path = file_path.with_suffix('.backup.csv')
                df.to_csv(backup_path, encoding='utf-8-sig', index=False)
                print(f"ğŸ’¾ å·²åˆ›å»ºå¤‡ä»½: {backup_path.name}")
            
            # ç§»é™¤å¤šä½™åˆ—
            df_cleaned = df.drop(columns=existing_columns_to_remove)
            print(f"ğŸ—‘ï¸ å·²ç§»é™¤åˆ— ({len(existing_columns_to_remove)}): {existing_columns_to_remove}")
            
            # ä¼˜åŒ–åˆ—é¡ºåº
            df_cleaned = optimize_column_order(df_cleaned)
            
            # æ•°æ®ç±»å‹ä¼˜åŒ–
            df_cleaned = optimize_data_types(df_cleaned)
            
            # ä¿å­˜æ¸…ç†åçš„æ–‡ä»¶
            df_cleaned.to_csv(file_path, encoding='utf-8-sig', index=False)
            
            print(f"âœ… æ¸…ç†å®Œæˆ: {original_columns}åˆ— â†’ {len(df_cleaned.columns)}åˆ—")
            print(f"ğŸ’¾ å·²ä¿å­˜: {file_path.name}")
            
            return True
        else:
            print("â„¹ï¸ æœªå‘ç°éœ€è¦ç§»é™¤çš„åˆ—")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        return False

def optimize_column_order(df):
    """ä¼˜åŒ–åˆ—é¡ºåº"""
    preferred_order = [
        'open_time',           # æ—¶é—´
        'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·',  # OHLC
        'æˆäº¤é‡', 'æˆäº¤é¢', 'æˆäº¤ç¬”æ•°',          # æˆäº¤é‡æ•°æ®
        'ä¸»åŠ¨ä¹°å…¥é‡', 'ä¸»åŠ¨ä¹°å…¥é¢',             # ä¹°å…¥æ•°æ®
        'MA20', 'MA50', 'MA_LONG',            # ç§»åŠ¨å¹³å‡çº¿
        'MACD', 'MACD_Signal', 'MACD_Hist',   # MACD
        'MACD_Long', 'MACD_Long_Signal',      # é•¿æœŸMACD
        'RSI', 'RSI_Secondary', 'RSI_Long',   # RSIç³»åˆ—
        'BB_Upper', 'BB_Middle', 'BB_Lower',  # å¸ƒæ—å¸¦
        'BB_Long_Upper', 'BB_Long_Middle', 'BB_Long_Lower',  # é•¿æœŸå¸ƒæ—å¸¦
        'Stoch_SlowK', 'Stoch_SlowD',         # éšæœºæŒ‡æ ‡
        'OBV',                                # æˆäº¤é‡æŒ‡æ ‡
        'ATR', 'ATR_Long', 'ATR_Ratio',       # ATRç³»åˆ—
        'ADX'                                 # è¶‹åŠ¿æŒ‡æ ‡
    ]
    
    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    existing_preferred = [col for col in preferred_order if col in df.columns]
    other_columns = [col for col in df.columns if col not in preferred_order]
    new_column_order = existing_preferred + other_columns
    
    return df[new_column_order]

def optimize_data_types(df):
    """ä¼˜åŒ–æ•°æ®ç±»å‹"""
    # å°†float64è½¬æ¢ä¸ºfloat32ä»¥èŠ‚çœå†…å­˜
    numeric_columns = df.select_dtypes(include=['float64']).columns
    if len(numeric_columns) > 0:
        df[numeric_columns] = df[numeric_columns].astype('float32')
        print(f"ğŸ”§ å·²ä¼˜åŒ–{len(numeric_columns)}ä¸ªæ•°å€¼åˆ—çš„æ•°æ®ç±»å‹")
    
    return df

def clean_all_csv_files():
    """æ¸…ç†æ‰€æœ‰ç»„åˆæ•°æ®CSVæ–‡ä»¶"""
    print("ğŸ§¹ æ‰¹é‡æ¸…ç†ç»„åˆæ•°æ®CSVæ–‡ä»¶")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»„åˆæ•°æ®æ–‡ä»¶
    csv_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°ç»„åˆæ•°æ®CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªç»„åˆæ•°æ®æ–‡ä»¶:")
    for file in csv_files:
        print(f"   - {file.name}")
    
    cleaned_count = 0
    
    for csv_file in csv_files:
        # åˆ†ææ–‡ä»¶
        df, redundant_cols = analyze_csv_columns(csv_file)
        
        if df is not None and redundant_cols:
            # æ¸…ç†æ–‡ä»¶
            if clean_csv_file(csv_file, backup=True):
                cleaned_count += 1
        else:
            print(f"â„¹ï¸ {csv_file.name} æ— éœ€æ¸…ç†")
    
    print(f"\nâœ… æ‰¹é‡æ¸…ç†å®Œæˆ: {cleaned_count}/{len(csv_files)} ä¸ªæ–‡ä»¶å·²æ¸…ç†")

def verify_cleaned_files():
    """éªŒè¯æ¸…ç†åçš„æ–‡ä»¶"""
    print("\nğŸ” éªŒè¯æ¸…ç†ç»“æœ")
    print("=" * 60)
    
    csv_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¤šä½™åˆ—
            unwanted_columns = ['BB_Squeeze', 'BB_Width', 'MA8', 'MA21', 'MA55']
            found_unwanted = [col for col in unwanted_columns if col in df.columns]
            
            if found_unwanted:
                print(f"âš ï¸ {csv_file.name}: ä»åŒ…å«å¤šä½™åˆ— {found_unwanted}")
            else:
                print(f"âœ… {csv_file.name}: æ¸…ç†å®Œæˆ ({len(df.columns)}åˆ—)")
                
        except Exception as e:
            print(f"âŒ {csv_file.name}: éªŒè¯å¤±è´¥ - {e}")

def show_column_comparison():
    """æ˜¾ç¤ºæ¸…ç†å‰åçš„åˆ—å¯¹æ¯”"""
    print("\nğŸ“Š åˆ—ç»“æ„å¯¹æ¯”")
    print("=" * 60)
    
    csv_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv"))
    backup_files = list(DATA_DIR.glob("*ç»„åˆæ•°æ®*.backup.csv"))
    
    for csv_file in csv_files:
        backup_file = csv_file.with_suffix('.backup.csv')
        
        if backup_file.exists():
            try:
                df_original = pd.read_csv(backup_file, encoding='utf-8-sig')
                df_cleaned = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                print(f"\nğŸ“ {csv_file.name}:")
                print(f"   æ¸…ç†å‰: {len(df_original.columns)}åˆ—")
                print(f"   æ¸…ç†å: {len(df_cleaned.columns)}åˆ—")
                print(f"   å‡å°‘: {len(df_original.columns) - len(df_cleaned.columns)}åˆ—")
                
                # æ˜¾ç¤ºè¢«ç§»é™¤çš„åˆ—
                removed_cols = set(df_original.columns) - set(df_cleaned.columns)
                if removed_cols:
                    print(f"   ç§»é™¤çš„åˆ—: {list(removed_cols)}")
                    
            except Exception as e:
                print(f"âŒ å¯¹æ¯”å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("BTCUSDT ç»„åˆæ•°æ®CSVæ¸…ç†å·¥å…·")
    print("=" * 80)
    print("åŠŸèƒ½: ç§»é™¤BB_Squeezeå’Œå…¶ä»–å¤šä½™æ•°æ®ï¼Œä¼˜åŒ–æ–‡ä»¶ç»“æ„")
    print("=" * 80)
    
    # 1. æ¸…ç†æ‰€æœ‰CSVæ–‡ä»¶
    clean_all_csv_files()
    
    # 2. éªŒè¯æ¸…ç†ç»“æœ
    verify_cleaned_files()
    
    # 3. æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    show_column_comparison()
    
    print("\n" + "=" * 80)
    print("æ¸…ç†æ€»ç»“:")
    print("âœ… å·²ç§»é™¤ BB_Squeeze åˆ— (å¸ƒæ—å¸¦æŒ¤å‹æ ‡å¿—)")
    print("âœ… å·²ç§»é™¤ BB_Width åˆ— (å¸ƒæ—å¸¦å®½åº¦)")
    print("âœ… å·²ç§»é™¤é‡å¤çš„MAåˆ— (MA8, MA21, MA55)")
    print("âœ… å·²ä¼˜åŒ–åˆ—é¡ºåº (æ ¸å¿ƒæŒ‡æ ‡å‰ç½®)")
    print("âœ… å·²ä¼˜åŒ–æ•°æ®ç±»å‹ (èŠ‚çœå†…å­˜)")
    print("âœ… å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶ (*.backup.csv)")
    print("\nå»ºè®®: å¦‚æœæ¸…ç†ç»“æœæ»¡æ„ï¼Œå¯ä»¥åˆ é™¤å¤‡ä»½æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´")

if __name__ == "__main__":
    main()
