"""
éªŒè¯ç²¾ç®€æ•°æ®æ–‡ä»¶
"""

import pandas as pd
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from config import DATA_DIR

def verify_streamlined_files():
    """éªŒè¯æ‰€æœ‰ç²¾ç®€æ–‡ä»¶"""
    print("ğŸ” éªŒè¯ç²¾ç®€æ•°æ®æ–‡ä»¶")
    print("=" * 80)
    
    # æŸ¥æ‰¾ç²¾ç®€æ–‡ä»¶
    streamlined_files = list(DATA_DIR.glob("*_streamlined.csv"))
    
    if not streamlined_files:
        print("âŒ æœªæ‰¾åˆ°ç²¾ç®€æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(streamlined_files)} ä¸ªç²¾ç®€æ–‡ä»¶:")
    
    for file in streamlined_files:
        print(f"\nğŸ“Š éªŒè¯æ–‡ä»¶: {file.name}")
        print("-" * 60)
        
        try:
            df = pd.read_csv(file, encoding='utf-8-sig')
            
            # åŸºæœ¬ä¿¡æ¯
            print(f"   æ•°æ®è¡Œæ•°: {len(df)}")
            print(f"   åˆ—æ•°: {len(df.columns)}")
            print(f"   æ–‡ä»¶å¤§å°: {file.stat().st_size / 1024:.1f} KB")
            
            # æ£€æŸ¥æ ¸å¿ƒåˆ—
            core_columns = ['æ”¶ç›˜ä»·', 'MA20', 'RSI', 'MACD', 'ATR']
            missing_core = [col for col in core_columns if col not in df.columns]
            if missing_core:
                print(f"   âš ï¸ ç¼ºå¤±æ ¸å¿ƒåˆ—: {missing_core}")
            else:
                print(f"   âœ… æ ¸å¿ƒæŠ€æœ¯æŒ‡æ ‡å®Œæ•´")
            
            # æ£€æŸ¥æ–æ³¢é‚£å¥‘å…³é”®æ°´å¹³
            fib_key_levels = ['Fib_Ret_0.382', 'Fib_Ret_0.500', 'Fib_Ret_0.618']
            fib_present = [col for col in fib_key_levels if col in df.columns]
            print(f"   ğŸ”¢ æ–æ³¢é‚£å¥‘å…³é”®æ°´å¹³: {len(fib_present)}/3")
            
            for col in fib_present:
                valid_count = df[col].notna().sum()
                if valid_count > 0:
                    latest_value = df[col].dropna().iloc[-1]
                    print(f"      {col}: {valid_count}/{len(df)} - æœ€æ–°: ${latest_value:.2f}")
            
            # æ•°æ®å®Œæ•´æ€§
            null_counts = df.isnull().sum()
            high_null_cols = null_counts[null_counts > len(df) * 0.3].index.tolist()
            if high_null_cols:
                print(f"   âš ï¸ é«˜ç©ºå€¼åˆ— (>30%): {high_null_cols}")
            else:
                print(f"   âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
            
            # æ˜¾ç¤ºåˆ—ç»“æ„
            print(f"   ğŸ“‹ åˆ—ç»“æ„: {list(df.columns)}")
            
        except Exception as e:
            print(f"   âŒ éªŒè¯å¤±è´¥: {e}")

def compare_original_vs_streamlined():
    """å¯¹æ¯”åŸå§‹æ–‡ä»¶ä¸ç²¾ç®€æ–‡ä»¶"""
    print(f"\nğŸ“Š åŸå§‹æ–‡ä»¶ vs ç²¾ç®€æ–‡ä»¶å¯¹æ¯”")
    print("=" * 80)
    
    # æŸ¥æ‰¾é…å¯¹æ–‡ä»¶
    original_files = [f for f in DATA_DIR.glob("*ç»„åˆæ•°æ®*.csv") if not f.name.endswith('_streamlined.csv') and not f.name.endswith('.backup.csv')]
    streamlined_files = list(DATA_DIR.glob("*_streamlined.csv"))
    
    comparison_data = []
    
    for original_file in original_files:
        # æŸ¥æ‰¾å¯¹åº”çš„ç²¾ç®€æ–‡ä»¶
        base_name = original_file.stem
        streamlined_name = f"{base_name}_streamlined.csv"
        streamlined_file = DATA_DIR / streamlined_name
        
        if streamlined_file.exists():
            try:
                df_original = pd.read_csv(original_file, encoding='utf-8-sig')
                df_streamlined = pd.read_csv(streamlined_file, encoding='utf-8-sig')
                
                original_size = original_file.stat().st_size / 1024
                streamlined_size = streamlined_file.stat().st_size / 1024
                size_reduction = (original_size - streamlined_size) / original_size * 100
                
                comparison_data.append({
                    'æ–‡ä»¶': original_file.name.replace('BTCUSDT_', '').replace('ç»„åˆæ•°æ®_20250720.csv', ''),
                    'åŸå§‹åˆ—æ•°': len(df_original.columns),
                    'ç²¾ç®€åˆ—æ•°': len(df_streamlined.columns),
                    'å‡å°‘åˆ—æ•°': len(df_original.columns) - len(df_streamlined.columns),
                    'åŸå§‹å¤§å°(KB)': f"{original_size:.1f}",
                    'ç²¾ç®€å¤§å°(KB)': f"{streamlined_size:.1f}",
                    'ç©ºé—´èŠ‚çœ': f"{size_reduction:.1f}%"
                })
                
            except Exception as e:
                print(f"âŒ å¯¹æ¯”å¤±è´¥ {original_file.name}: {e}")
    
    if comparison_data:
        comparison_df = pd.DataFrame(comparison_data)
        print(comparison_df.to_string(index=False))
    else:
        print("âŒ æœªæ‰¾åˆ°å¯å¯¹æ¯”çš„æ–‡ä»¶")

def show_streamlined_structure():
    """æ˜¾ç¤ºç²¾ç®€æ–‡ä»¶çš„æ ‡å‡†ç»“æ„"""
    print(f"\nğŸ“‹ ç²¾ç®€æ–‡ä»¶æ ‡å‡†ç»“æ„")
    print("=" * 80)
    
    structure = {
        'ç±»åˆ«': [
            'åŸºç¡€æ•°æ®', 'ç§»åŠ¨å¹³å‡', 'MACD', 'RSI', 'å¸ƒæ—å¸¦', 
            'æ–æ³¢é‚£å¥‘å…³é”®', 'æ–æ³¢é‚£å¥‘æ‰©å±•', 'æ–æ³¢é‚£å¥‘ä¿¡å·', 'å…¶ä»–æ ¸å¿ƒ'
        ],
        'åˆ—æ•°': [6, 2, 3, 1, 3, 3, 2, 2, 3],
        'å…·ä½“æŒ‡æ ‡': [
            'open_time, OHLC, æˆäº¤é‡',
            'MA20, MA50',
            'MACD, MACD_Signal, MACD_Hist',
            'RSI',
            'BB_Upper, BB_Middle, BB_Lower',
            'Fib_Ret_0.382, 0.500, 0.618',
            'Fib_Ext_1.272, 1.618',
            'Fib_Trend, Fib_Signal',
            'ATR, ADX, OBV'
        ]
    }
    
    structure_df = pd.DataFrame(structure)
    print(structure_df.to_string(index=False))
    
    print(f"\nâœ… ç²¾ç®€åŸåˆ™:")
    print("   1. ä¿ç•™æœ€æ ¸å¿ƒçš„æŠ€æœ¯æŒ‡æ ‡")
    print("   2. é‡ç‚¹ä¿ç•™æ–æ³¢é‚£å¥‘å…³é”®æ°´å¹³ (38.2%, 50%, 61.8%)")
    print("   3. ç§»é™¤å†—ä½™å’Œæ¬¡è¦æŒ‡æ ‡")
    print("   4. ä¼˜åŒ–æ–‡ä»¶å¤§å°å’Œä¼ è¾“æ•ˆç‡")
    print("   5. é€‚åˆDeepSeek AIå¿«é€Ÿåˆ†æ")

def generate_usage_recommendations():
    """ç”Ÿæˆä½¿ç”¨å»ºè®®"""
    print(f"\nğŸ’¡ ç²¾ç®€æ–‡ä»¶ä½¿ç”¨å»ºè®®")
    print("=" * 80)
    
    print("ğŸ¯ é€‚ç”¨åœºæ™¯:")
    print("   âœ… å‘é€ç»™DeepSeek AIåˆ†æ")
    print("   âœ… çŸ­çº¿å’Œæ—¥å†…äº¤æ˜“å†³ç­–")
    print("   âœ… å¿«é€ŸæŠ€æœ¯åˆ†æ")
    print("   âœ… ç§»åŠ¨è®¾å¤‡æŸ¥çœ‹")
    print("   âœ… ç½‘ç»œä¼ è¾“ä¼˜åŒ–")
    
    print(f"\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡è§£è¯»:")
    print("   â€¢ MA20/MA50: çŸ­ä¸­æœŸè¶‹åŠ¿")
    print("   â€¢ MACD: åŠ¨é‡å’Œè¶‹åŠ¿å˜åŒ–")
    print("   â€¢ RSI: è¶…ä¹°è¶…å–çŠ¶æ€")
    print("   â€¢ å¸ƒæ—å¸¦: ä»·æ ¼é€šé“å’Œæ³¢åŠ¨ç‡")
    print("   â€¢ æ–æ³¢é‚£å¥‘38.2%: å…³é”®å›è°ƒä½")
    print("   â€¢ æ–æ³¢é‚£å¥‘50%: é»„é‡‘åˆ†å‰²ç‚¹")
    print("   â€¢ æ–æ³¢é‚£å¥‘61.8%: é»„é‡‘æ¯”ä¾‹")
    print("   â€¢ ATR: æ³¢åŠ¨ç‡å’Œæ­¢æŸå‚è€ƒ")
    
    print(f"\nğŸš€ ä¸DeepSeek AIç»“åˆ:")
    print("   1. ä¸Šä¼ ç²¾ç®€æ–‡ä»¶ï¼Œåˆ†æé€Ÿåº¦æ›´å¿«")
    print("   2. AIèƒ½ç²¾å‡†è¯†åˆ«å…³é”®æ”¯æ’‘é˜»åŠ›")
    print("   3. åŸºäºæ–æ³¢é‚£å¥‘æ°´å¹³çš„äº¤æ˜“å»ºè®®")
    print("   4. ç»“åˆå¤šä¸ªæŒ‡æ ‡çš„ç»¼åˆåˆ†æ")

def main():
    """ä¸»å‡½æ•°"""
    print("BTCUSDT ç²¾ç®€æ•°æ®éªŒè¯å·¥å…·")
    print("=" * 80)
    
    # éªŒè¯ç²¾ç®€æ–‡ä»¶
    verify_streamlined_files()
    
    # å¯¹æ¯”åˆ†æ
    compare_original_vs_streamlined()
    
    # æ˜¾ç¤ºç»“æ„
    show_streamlined_structure()
    
    # ä½¿ç”¨å»ºè®®
    generate_usage_recommendations()
    
    print(f"\n" + "=" * 80)
    print("ğŸ‰ ç²¾ç®€æ•°æ®éªŒè¯å®Œæˆ!")
    print("âœ… ç²¾ç®€æ–‡ä»¶å·²å‡†å¤‡å°±ç»ª")
    print("âœ… ä¿ç•™äº†æœ€æ ¸å¿ƒçš„25ä¸ªæŠ€æœ¯æŒ‡æ ‡")
    print("âœ… æ–‡ä»¶å¤§å°å‡å°‘çº¦50%")
    print("âœ… é‡ç‚¹ä¿ç•™æ–æ³¢é‚£å¥‘å…³é”®æ°´å¹³")
    print("\nğŸ“ æ¨èä½¿ç”¨çš„ç²¾ç®€æ–‡ä»¶:")
    
    streamlined_files = list(DATA_DIR.glob("*_streamlined.csv"))
    for file in streamlined_files:
        print(f"   â€¢ {file.name}")

if __name__ == "__main__":
    main()
